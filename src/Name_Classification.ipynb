{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1Kk_3g60Jpu3ytijgIMcw7iIwEnOKjfvn","authorship_tag":"ABX9TyP+4Ez2iThU/O94Zhs1vHJq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"40e3590190004d15bba3ba1d415a6a0d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e708378d02043a39def31842380c0a1","IPY_MODEL_71ec213e1d6442cea8fe62d9662bc93d","IPY_MODEL_4a611516447d48dea80e73b47981e644"],"layout":"IPY_MODEL_a8a90317116d412abf7548c985025a69"}},"9e708378d02043a39def31842380c0a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b74622285c2487db19f469b68907225","placeholder":"​","style":"IPY_MODEL_caceec925dc34f8d96793d979434bb77","value":"Map: 100%"}},"71ec213e1d6442cea8fe62d9662bc93d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6885ca96bce14e7c9d59e7f3f579e681","max":18651,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60bd79e1c8da4ab5b9349e2db2c14000","value":18651}},"4a611516447d48dea80e73b47981e644":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cb8f49f3a3e4fb4b0618a557b30c443","placeholder":"​","style":"IPY_MODEL_486e34d4707b4e20a97779b4f261c3c3","value":" 18651/18651 [00:07&lt;00:00, 2855.08 examples/s]"}},"a8a90317116d412abf7548c985025a69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b74622285c2487db19f469b68907225":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caceec925dc34f8d96793d979434bb77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6885ca96bce14e7c9d59e7f3f579e681":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60bd79e1c8da4ab5b9349e2db2c14000":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1cb8f49f3a3e4fb4b0618a557b30c443":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"486e34d4707b4e20a97779b4f261c3c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f2b4e0f43dc4579859bf2b929614c52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b168b832174e41a98eefd561003cc765","IPY_MODEL_b676f0d7e2ad4860b4678b6ff5585622","IPY_MODEL_c1ea40f6031c46c1852eb4a4a5deaa54"],"layout":"IPY_MODEL_bb8bdc69e9c040eaa88fb917a348bffa"}},"b168b832174e41a98eefd561003cc765":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c41afd1f97e4f238c04bdfc026297be","placeholder":"​","style":"IPY_MODEL_1e63e95e4bc84c6fac887af97fdc5bbd","value":"Map: 100%"}},"b676f0d7e2ad4860b4678b6ff5585622":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6cce44e7fc8439eb236d79b50eb9c88","max":3997,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4ee3a275a314481a46e390b1d7b3f86","value":3997}},"c1ea40f6031c46c1852eb4a4a5deaa54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e9f4466f6644b4daf7fff98732875e1","placeholder":"​","style":"IPY_MODEL_2c3e6ec2cef8429b9dfaaaa11bd0f0ca","value":" 3997/3997 [00:01&lt;00:00, 3228.20 examples/s]"}},"bb8bdc69e9c040eaa88fb917a348bffa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c41afd1f97e4f238c04bdfc026297be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e63e95e4bc84c6fac887af97fdc5bbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6cce44e7fc8439eb236d79b50eb9c88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4ee3a275a314481a46e390b1d7b3f86":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e9f4466f6644b4daf7fff98732875e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c3e6ec2cef8429b9dfaaaa11bd0f0ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68b9052f5d124d3585f58f4999f85b9e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9acd798267f4c8c8d15261caea2838e","IPY_MODEL_00db02d64e684144bd346646791dd647","IPY_MODEL_b68a6c5a4a0d43efb82fe414bd58b573"],"layout":"IPY_MODEL_d09dbd657fe841eb84368d7c6c71dde8"}},"d9acd798267f4c8c8d15261caea2838e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7efd06083234c06aa548af64ffa67a0","placeholder":"​","style":"IPY_MODEL_418268db53f544f9bf199d3a18ec7ddf","value":"Map: 100%"}},"00db02d64e684144bd346646791dd647":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_70b0e591496e4c84a9428f1e98fde5a6","max":3998,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac3187eb81d145d1a715c747b1143512","value":3998}},"b68a6c5a4a0d43efb82fe414bd58b573":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5b848ce122c4129b53319380089ac92","placeholder":"​","style":"IPY_MODEL_bed72f99283647619d24c2cd92063870","value":" 3998/3998 [00:00&lt;00:00, 4237.58 examples/s]"}},"d09dbd657fe841eb84368d7c6c71dde8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7efd06083234c06aa548af64ffa67a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"418268db53f544f9bf199d3a18ec7ddf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70b0e591496e4c84a9428f1e98fde5a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac3187eb81d145d1a715c747b1143512":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5b848ce122c4129b53319380089ac92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bed72f99283647619d24c2cd92063870":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Phase 1: Data Preprocessing and Feature Engineering"],"metadata":{"id":"Vrxcph8dIfR1"}},{"cell_type":"markdown","source":["Step 1: Load → Target → Clean names"],"metadata":{"id":"cqWCgdIvGpic"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"QmovT9NcZ4r2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758479966327,"user_tz":240,"elapsed":3314,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"fe5f72dd-054f-4fcd-baab-9ccd1fad4fb4"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3508182767.py:83: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(CSV_PATH, usecols=usecols, chunksize=CHUNK_SIZE,\n"]},{"output_type":"stream","name":"stdout","text":["Ingested chunk: 28,694 rows | Total read: 28,694\n","SQLite rows (deduped): 26,646\n"]}],"source":["\"\"\"\n","STEP 1:\n","* Builds is_south_asian using all present SA indicator columns.\n","* Normalizes names (unicode, spacing, case) without throwing information away.\n","* Extracts first_clean and last_clean so we can handle mixed-origin names later.\n","* Adds simple diagnostics (share of SA, avg lengths).\n","\n","\"\"\"\n","\n","import re, unicodedata, sqlite3\n","from pathlib import Path\n","import pandas as pd\n","\n","# ======= CONFIG =======\n","CSV_PATH   = Path(\"/mnt/data/americans_by_descent.csv\")   # update path if needed\n","DB_PATH    = Path(\"./sa_names.db\")                        # SQLite file\n","CHUNK_SIZE = 50_000                                       # adjust to your RAM (e.g., 20k–200k)\n","# ======================\n","\n","# 1) Peek the header to select only needed columns\n","hdr = pd.read_csv(CSV_PATH, nrows=0, encoding=\"ISO-8859-1\", low_memory=False)\n","all_cols = list(map(str, hdr.columns))\n","\n","south_asian_candidates = [\n","    \"South Asian\",\n","    \"Bangladeshi\",\"Bhutanese\",\"Indian\",\"Nepali\",\"Pakistani\",\"Sri Lankan\",\n","    \"Bengali\",\"Gujarati\",\"Kashmiri\",\"Malayali\",\"Punjabi\",\"Sindhi\",\n","    \"Sinhala\",\"Sri Lankan Tamil\",\"Tamil\",\"Telugu\",\"Marathi\",\"Kannadiga\",\n","    \"Tuluva\",\"Rajasthani\",\"Bihari\",\"Awadhi\",\"Bhojpuri\",\"Maithil\",\"Haryanvi\",\n","]\n","present_sa_cols = [c for c in south_asian_candidates if c in all_cols]\n","\n","usecols = [c for c in [\"id\",\"name\"] if c in all_cols] + present_sa_cols\n","assert \"name\" in usecols, \"The CSV must contain a 'name' column.\"\n","\n","# 2) Cleaning helpers\n","def normalize_unicode(s: str) -> str:\n","    if not isinstance(s, str): return \"\"\n","    s = (s.replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\")\n","           .replace(\"\\u201C\", '\"').replace(\"\\u201D\", '\"')\n","           .replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n","           .replace(\"\\u00A0\", \" \"))\n","    return unicodedata.normalize(\"NFKC\", s)\n","\n","def ascii_fold(s: str) -> str:\n","    return unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n","\n","def clean_name(raw: str) -> str:\n","    if not isinstance(raw, str): return \"\"\n","    s = normalize_unicode(raw).strip().lower()\n","    s = re.sub(r\"[,\\u200b]\", \" \", s)\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","\n","def split_first_last(cleaned: str):\n","    if not cleaned: return \"\", \"\"\n","    parts = cleaned.split()\n","    if len(parts) == 1: return parts[0], \"\"\n","    return parts[0], parts[-1]\n","\n","# 3) SQLite: create table with UNIQUE(name_ascii) to dedup at ingest\n","con = sqlite3.connect(DB_PATH)\n","cur = con.cursor()\n","cur.execute(\"\"\"\n","CREATE TABLE IF NOT EXISTS names_preprocessed (\n","  id                TEXT,\n","  name              TEXT NOT NULL,\n","  name_clean        TEXT NOT NULL,\n","  name_ascii        TEXT NOT NULL,\n","  first_clean       TEXT,\n","  last_clean        TEXT,\n","  is_south_asian    INTEGER NOT NULL,\n","  name_len          INTEGER,\n","  word_count        INTEGER,\n","  PRIMARY KEY (name_ascii)                -- unique key for dedup\n",");\n","\"\"\")\n","con.commit()\n","\n","# 4) Stream the CSV in chunks\n","total_rows = 0\n","inserted_rows = 0\n","for chunk in pd.read_csv(CSV_PATH, usecols=usecols, chunksize=CHUNK_SIZE,\n","                         encoding=\"ISO-8859-1\", low_memory=True):\n","    total_rows += len(chunk)\n","\n","    # build target\n","    if present_sa_cols:\n","        chunk[\"is_south_asian\"] = (chunk[present_sa_cols].fillna(0).sum(axis=1) > 0).astype(\"int8\")\n","        # free unneeded SA columns immediately to save RAM\n","        chunk.drop(columns=present_sa_cols, inplace=True, errors=\"ignore\")\n","    else:\n","        chunk[\"is_south_asian\"] = 0\n","\n","    # clean + structure\n","    chunk[\"name_clean\"] = chunk[\"name\"].map(clean_name)\n","    fl = chunk[\"name_clean\"].map(split_first_last)\n","    chunk[\"first_clean\"] = fl.map(lambda t: t[0])\n","    chunk[\"last_clean\"]  = fl.map(lambda t: t[1])\n","    chunk[\"name_ascii\"]  = chunk[\"name_clean\"].map(ascii_fold)\n","    chunk[\"name_len\"]    = chunk[\"name_clean\"].str.len().astype(\"Int32\")\n","    chunk[\"word_count\"]  = chunk[\"name_clean\"].str.split().str.len().astype(\"Int32\")\n","\n","    # keep only columns we store\n","    keep = [c for c in [\"id\",\"name\",\"name_clean\",\"name_ascii\",\"first_clean\",\"last_clean\",\n","                        \"is_south_asian\",\"name_len\",\"word_count\"] if c in chunk.columns]\n","    chunk = chunk[keep]\n","\n","    # insert with IGNORE to skip duplicates by PRIMARY KEY(name_ascii)\n","    # (executemany is fastest; wrap in transaction)\n","    recs = list(chunk.itertuples(index=False, name=None))\n","    cur.executemany(f\"\"\"\n","        INSERT OR IGNORE INTO names_preprocessed\n","        ({\",\".join(keep)}) VALUES ({\",\".join([\"?\"]*len(keep))})\n","    \"\"\", recs)\n","    con.commit()\n","\n","    # stats\n","    # SQLite change count isn’t trivial, so we can estimate on next loop if desired\n","    # (or query COUNT(*) occasionally)\n","    print(f\"Ingested chunk: {len(recs):,} rows | Total read: {total_rows:,}\")\n","\n","# final count\n","cnt = cur.execute(\"SELECT COUNT(*) FROM names_preprocessed\").fetchone()[0]\n","print(f\"SQLite rows (deduped): {cnt:,}\")\n","con.close()"]},{"cell_type":"markdown","source":["Step 2 — Exact Dedup + Stratified Splits"],"metadata":{"id":"s5dA4vj4HDlX"}},{"cell_type":"code","source":["# STEP 2: We deduplicate by a normalized ASCII name to prevent duplicate names leaking across splits (optimistic test scores). Then we build 70/15/15 stratified splits on is_south_asian.s\n","\n","import sqlite3\n","\n","DB_PATH = \"./sa_names.db\"\n","con = sqlite3.connect(DB_PATH)\n","cur = con.cursor()\n","\n","# Make sure we can use window functions efficiently\n","cur.execute(\"PRAGMA journal_mode=WAL;\")\n","cur.execute(\"PRAGMA temp_store=MEMORY;\")\n","cur.execute(\"PRAGMA synchronous=NORMAL;\")\n","\n","# 1) Add a persistent random key for deterministic partitioning (only fills if missing)\n","try:\n","    cur.execute(\"ALTER TABLE names_preprocessed ADD COLUMN rnd TEXT;\")\n","    con.commit()\n","except sqlite3.OperationalError:\n","    # column exists\n","    pass\n","\n","cur.execute(\"\"\"\n","UPDATE names_preprocessed\n","SET rnd = COALESCE(rnd, lower(hex(randomblob(16))));\n","\"\"\")\n","con.commit()\n","\n","# 2) Build splits table with ~70/15/15 per class (stratified by is_south_asian)\n","cur.execute(\"DROP TABLE IF EXISTS names_splits;\")\n","cur.execute(\"\"\"\n","CREATE TABLE names_splits AS\n","WITH ranked AS (\n","  SELECT\n","    name_ascii,\n","    is_south_asian,\n","    rnd,\n","    ROW_NUMBER() OVER (PARTITION BY is_south_asian ORDER BY rnd) AS rnk,\n","    COUNT(*)    OVER (PARTITION BY is_south_asian)               AS n\n","  FROM names_preprocessed\n","),\n","assigned AS (\n","  SELECT\n","    name_ascii, is_south_asian, rnd, rnk, n,\n","    CASE\n","      WHEN rnk <= 0.70*n THEN 'train'\n","      WHEN rnk <= 0.85*n THEN 'val'\n","      ELSE 'test'\n","    END AS split\n","  FROM ranked\n",")\n","SELECT * FROM assigned;\n","\"\"\")\n","con.commit()\n","\n","# 3) Quick sanity: counts by split and class\n","def split_stats(tag):\n","    cnt = cur.execute(\n","        \"SELECT COUNT(*) FROM names_splits WHERE split=?;\", (tag,)\n","    ).fetchone()[0]\n","    sa  = cur.execute(\n","        \"SELECT COALESCE(SUM(is_south_asian),0) FROM names_splits WHERE split=?;\", (tag,)\n","    ).fetchone()[0]\n","    return cnt, sa, cnt-sa\n","\n","train_cnt, train_sa, train_non = split_stats(\"train\")\n","val_cnt,   val_sa,   val_non   = split_stats(\"val\")\n","test_cnt,  test_sa,  test_non  = split_stats(\"test\")\n","\n","print({\n","    \"split_sizes\": {\n","        \"train\": train_cnt, \"val\": val_cnt, \"test\": test_cnt\n","    },\n","    \"class_balance\": {\n","        \"train\": {\"sa\": train_sa, \"non_sa\": train_non},\n","        \"val\":   {\"sa\": val_sa,   \"non_sa\": val_non},\n","        \"test\":  {\"sa\": test_sa,  \"non_sa\": test_non},\n","    }\n","})\n","\n","# 4) Optional: verify no overlap between splits\n","overlap = cur.execute(\"\"\"\n","SELECT COUNT(*) FROM (\n","  SELECT name_ascii, COUNT(DISTINCT split) AS k\n","  FROM names_splits\n","  GROUP BY name_ascii\n","  HAVING k > 1\n",");\n","\"\"\").fetchone()[0]\n","print({\"name_overlap_across_splits\": overlap})\n","\n","con.close()"],"metadata":{"id":"838WdfnHG2s3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758479967518,"user_tz":240,"elapsed":1192,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"a5585465-cc10-4d3c-873a-c8e62fb08e7c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'split_sizes': {'train': 18651, 'val': 3997, 'test': 3998}, 'class_balance': {'train': {'sa': 1696, 'non_sa': 16955}, 'val': {'sa': 364, 'non_sa': 3633}, 'test': {'sa': 364, 'non_sa': 3634}}}\n","{'name_overlap_across_splits': 0}\n"]}]},{"cell_type":"markdown","source":["Step 3 — Build TF-IDF char n-gram features, then transform Val/Test Set"],"metadata":{"id":"Cbj12WWwDXSZ"}},{"cell_type":"code","source":["# STEP 3: TF-IDF char n-grams for full/first/last views (fit on train only)\n","\n","\"\"\"\n","* Loads the train/val/test splits directly from SQLite (low memory).\n","* Fits three TF-IDF vectorizers (char n-grams 2–5) on train only.\n","* Transforms val/test with those exact vectorizers.\n","* Saves vectorizers and matrices to disk for modeling in the next step.\n","\"\"\"\n","\n","from pathlib import Path\n","import sqlite3, json, joblib\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy import sparse\n","\n","DB_PATH = \"./sa_names.db\"\n","ART_DIR = Path(\"./artifacts_tfidf\")\n","ART_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# ---------- helpers ----------\n","def fetch_split(con, split: str, cols=('name_ascii','first_clean','last_clean','is_south_asian')):\n","    q = f\"\"\"\n","    SELECT {\",\".join(cols)}\n","    FROM names_preprocessed\n","    WHERE name_ascii IN (\n","        SELECT name_ascii FROM names_splits WHERE split=?\n","    );\n","    \"\"\"\n","    return pd.read_sql_query(q, con, params=(split,))\n","\n","def fit_vec(train_series):\n","    # Char n-grams capture name sub-structure well; min_df=2 reduces noise.\n","    vec = TfidfVectorizer(\n","        analyzer=\"char\",\n","        ngram_range=(2,5),\n","        min_df=2,\n","        lowercase=False,   # we already normalized to lowercase\n","        norm=\"l2\"\n","    )\n","    return vec.fit(train_series)\n","\n","def transform_to_npz(vec, series, path_npz):\n","    X = vec.transform(series.fillna(\"\"))\n","    sparse.save_npz(path_npz, X)\n","    return X.shape\n","\n","# ---------- load splits ----------\n","con = sqlite3.connect(DB_PATH)\n","train_df = fetch_split(con, \"train\")\n","val_df   = fetch_split(con, \"val\")\n","test_df  = fetch_split(con, \"test\")\n","con.close()\n","\n","# Sanity checks\n","assert {\"name_ascii\",\"first_clean\",\"last_clean\",\"is_south_asian\"} <= set(train_df.columns)\n","print({\n","    \"train_rows\": len(train_df),\n","    \"val_rows\": len(val_df),\n","    \"test_rows\": len(test_df),\n","    \"sa_rate_train_pct\": round(train_df[\"is_south_asian\"].mean()*100, 3),\n","    \"sa_rate_val_pct\": round(val_df[\"is_south_asian\"].mean()*100, 3),\n","    \"sa_rate_test_pct\": round(test_df[\"is_south_asian\"].mean()*100, 3),\n","})\n","\n","# ---------- fit vectorizers on TRAIN only ----------\n","vec_full  = fit_vec(train_df[\"name_ascii\"])\n","vec_first = fit_vec(train_df[\"first_clean\"])\n","vec_last  = fit_vec(train_df[\"last_clean\"])\n","\n","# Save vectorizers\n","joblib.dump(vec_full,  ART_DIR / \"vec_full_tfidf.joblib\")\n","joblib.dump(vec_first, ART_DIR / \"vec_first_tfidf.joblib\")\n","joblib.dump(vec_last,  ART_DIR / \"vec_last_tfidf.joblib\")\n","\n","# ---------- transform & save matrices ----------\n","shapes = {}\n","\n","# full\n","shapes[\"Xtr_full\"] = transform_to_npz(vec_full,  train_df[\"name_ascii\"], ART_DIR / \"Xtr_full.npz\")\n","shapes[\"Xva_full\"] = transform_to_npz(vec_full,  val_df[\"name_ascii\"],   ART_DIR / \"Xva_full.npz\")\n","shapes[\"Xte_full\"] = transform_to_npz(vec_full,  test_df[\"name_ascii\"],  ART_DIR / \"Xte_full.npz\")\n","\n","# first\n","shapes[\"Xtr_first\"] = transform_to_npz(vec_first, train_df[\"first_clean\"], ART_DIR / \"Xtr_first.npz\")\n","shapes[\"Xva_first\"] = transform_to_npz(vec_first, val_df[\"first_clean\"],   ART_DIR / \"Xva_first.npz\")\n","shapes[\"Xte_first\"] = transform_to_npz(vec_first, test_df[\"first_clean\"],  ART_DIR / \"Xte_first.npz\")\n","\n","# last\n","shapes[\"Xtr_last\"] = transform_to_npz(vec_last,  train_df[\"last_clean\"], ART_DIR / \"Xtr_last.npz\")\n","shapes[\"Xva_last\"] = transform_to_npz(vec_last,  val_df[\"last_clean\"],   ART_DIR / \"Xva_last.npz\")\n","shapes[\"Xte_last\"] = transform_to_npz(vec_last,  test_df[\"last_clean\"],  ART_DIR / \"Xte_last.npz\")\n","\n","report = {\n","    \"tfidf_vocab_sizes\": {\n","        \"full\":  len(vec_full.vocabulary_),\n","        \"first\": len(vec_first.vocabulary_),\n","        \"last\":  len(vec_last.vocabulary_),\n","    },\n","    \"matrix_shapes\": shapes\n","}\n","print(report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vswn_36ODfov","executionInfo":{"status":"ok","timestamp":1758479984876,"user_tz":240,"elapsed":17350,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"bff16e92-b1d9-4bf2-e6c2-99e0265435a8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'train_rows': 18651, 'val_rows': 3997, 'test_rows': 3998, 'sa_rate_train_pct': np.float64(9.093), 'sa_rate_val_pct': np.float64(9.107), 'sa_rate_test_pct': np.float64(9.105)}\n","{'tfidf_vocab_sizes': {'full': 57322, 'first': 10835, 'last': 23277}, 'matrix_shapes': {'Xtr_full': (18651, 57322), 'Xva_full': (3997, 57322), 'Xte_full': (3998, 57322), 'Xtr_first': (18651, 10835), 'Xva_first': (3997, 10835), 'Xte_first': (3998, 10835), 'Xtr_last': (18651, 23277), 'Xva_last': (3997, 23277), 'Xte_last': (3998, 23277)}}\n"]}]},{"cell_type":"markdown","source":["Step 4: Logistic Regression Baseline + Evaluation Harness"],"metadata":{"id":"tPlJIAD1F8lM"}},{"cell_type":"code","source":["# STEP 4: Logistic Regression baselines for full / first / last + fusion\n","\n","\"\"\"\n","* Memory-safe: loads splits from SQLite\n","* Class imbalance handled via class_weight=\"balanced\"\n","* Reports PR-AUC, ROC-AUC, and thresholded metrics (max-F1, high-precision, high-recall)\n","* Saves models/vectorizers to ./artifacts_baseline\n","\"\"\"\n","\n","from pathlib import Path\n","import json\n","import sqlite3\n","import joblib\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import (\n","    average_precision_score, roc_auc_score,\n","    precision_recall_curve, classification_report, confusion_matrix\n",")\n","from scipy import sparse\n","\n","# -------------------\n","# Config\n","# -------------------\n","DB_PATH = \"./sa_names.db\"\n","ART_DIR = Path(\"./artifacts_baseline\")\n","ART_DIR.mkdir(parents=True, exist_ok=True)\n","\n","NGRAM_RANGE = (2, 5)\n","MIN_DF_DEFAULT = 2      # reduce noise, consistent with prior step\n","MIN_DF_FALLBACK = 1     # fallback if a view gets empty vocab\n","MAX_ITER = 400\n","SEED = 42\n","\n","# -------------------\n","# Helpers\n","# -------------------\n","def fetch_split(con, split: str, cols=('name_ascii','first_clean','last_clean','is_south_asian')):\n","    q = f\"\"\"\n","    SELECT {\",\".join(cols)}\n","    FROM names_preprocessed\n","    WHERE name_ascii IN (SELECT name_ascii FROM names_splits WHERE split=?);\n","    \"\"\"\n","    return pd.read_sql_query(q, con, params=(split,))\n","\n","def mk_vectorizer(min_df):\n","    return TfidfVectorizer(\n","        analyzer=\"char\",\n","        ngram_range=NGRAM_RANGE,\n","        min_df=min_df,\n","        lowercase=False,\n","        norm=\"l2\"\n","    )\n","\n","def fit_vec_safe(train_series, min_df=MIN_DF_DEFAULT):\n","    \"\"\"Fit TF-IDF, falling back to lower min_df if needed.\"\"\"\n","    try:\n","        vec = mk_vectorizer(min_df=min_df).fit(train_series)\n","    except ValueError as e:\n","        # Empty vocabulary likely; retry with fallback\n","        vec = mk_vectorizer(min_df=MIN_DF_FALLBACK).fit(train_series)\n","    return vec\n","\n","def train_logreg(X, y):\n","    # Class-weighted logistic regression for imbalance\n","    lr = LogisticRegression(\n","        penalty=\"l2\",\n","        solver=\"saga\",\n","        class_weight=\"balanced\",\n","        max_iter=MAX_ITER,\n","        n_jobs=-1,\n","        random_state=SEED,\n","        verbose=0,\n","    )\n","    lr.fit(X, y)\n","    return lr\n","\n","def eval_probs(y_true, p_scores, label=\"model\"):\n","    \"\"\"Return metrics dict + threshold table entries for max-F1, R>=0.90, P>=0.90.\"\"\"\n","    # Scores\n","    pr_auc = float(average_precision_score(y_true, p_scores))\n","    try:\n","        roc = float(roc_auc_score(y_true, p_scores))\n","    except ValueError:\n","        roc = float(\"nan\")\n","\n","    # Precision-Recall curve and best-F1 threshold\n","    precision, recall, thresholds = precision_recall_curve(y_true, p_scores)\n","    # precision_recall_curve returns n+1 points; make thresholds align\n","    thr = np.concatenate([thresholds, [1.0]])\n","\n","    f1 = np.where((precision + recall) > 0, 2 * precision * recall / (precision + recall), 0.0)\n","    best_idx = int(np.nanargmax(f1))\n","    best = {\n","        \"threshold\": float(thr[best_idx]),\n","        \"precision\": float(precision[best_idx]),\n","        \"recall\": float(recall[best_idx]),\n","        \"f1\": float(f1[best_idx]),\n","    }\n","\n","    # Threshold achieving at least target recall (choose highest precision under that constraint)\n","    def threshold_for_target_recall(target=0.90):\n","        ok = np.where(recall >= target)[0]\n","        if len(ok) == 0:\n","            return None\n","        i = ok[np.argmax(precision[ok])]  # among those, best precision\n","        return {\"threshold\": float(thr[i]), \"precision\": float(precision[i]), \"recall\": float(recall[i]), \"f1\": float(f1[i])}\n","\n","    # Threshold achieving at least target precision (choose highest recall under that constraint)\n","    def threshold_for_target_precision(target=0.90):\n","        ok = np.where(precision >= target)[0]\n","        if len(ok) == 0:\n","            return None\n","        i = ok[np.argmax(recall[ok])]\n","        return {\"threshold\": float(thr[i]), \"precision\": float(precision[i]), \"recall\": float(recall[i]), \"f1\": float(f1[i])}\n","\n","    at_r90 = threshold_for_target_recall(0.90)\n","    at_p90 = threshold_for_target_precision(0.90)\n","\n","    # Confusion matrix at best-F1 threshold\n","    y_hat = (p_scores >= best[\"threshold\"]).astype(int)\n","    cm = confusion_matrix(y_true, y_hat, labels=[0,1])\n","    report = classification_report(y_true, y_hat, labels=[0,1], target_names=[\"non-SA\",\"SA\"], output_dict=True)\n","\n","    return {\n","        \"label\": label,\n","        \"pr_auc\": pr_auc,\n","        \"roc_auc\": roc,\n","        \"best_f1\": best,\n","        \"p_at_recall_0.90\": at_r90,\n","        \"r_at_precision_0.90\": at_p90,\n","        \"confusion_matrix\": {\"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]), \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1])},\n","        \"classification_report_macro_f1\": float(report[\"macro avg\"][\"f1-score\"]),\n","        \"classification_report_SA_f1\": float(report[\"SA\"][\"f1-score\"]),\n","    }\n","\n","def to_csr(vec, series):\n","    return vec.transform(series.fillna(\"\"))\n","\n","# -------------------\n","# Load splits\n","# -------------------\n","con = sqlite3.connect(DB_PATH)\n","train_df = fetch_split(con, \"train\")\n","val_df   = fetch_split(con, \"val\")\n","test_df  = fetch_split(con, \"test\")\n","con.close()\n","\n","y_tr = train_df[\"is_south_asian\"].values.astype(int)\n","y_va = val_df[\"is_south_asian\"].values.astype(int)\n","y_te = test_df[\"is_south_asian\"].values.astype(int)\n","\n","# -------------------\n","# Vectorize three views (fit on TRAIN only)\n","# -------------------\n","vec_full  = fit_vec_safe(train_df[\"name_ascii\"])\n","vec_first = fit_vec_safe(train_df[\"first_clean\"])\n","vec_last  = fit_vec_safe(train_df[\"last_clean\"])\n","\n","# Save vectorizers\n","joblib.dump(vec_full,  ART_DIR / \"vec_full_tfidf.joblib\")\n","joblib.dump(vec_first, ART_DIR / \"vec_first_tfidf.joblib\")\n","joblib.dump(vec_last,  ART_DIR / \"vec_last_tfidf.joblib\")\n","\n","# Transform\n","Xtr_full,  Xva_full,  Xte_full  = to_csr(vec_full,  train_df[\"name_ascii\"]), to_csr(vec_full,  val_df[\"name_ascii\"]), to_csr(vec_full,  test_df[\"name_ascii\"])\n","Xtr_first, Xva_first, Xte_first = to_csr(vec_first, train_df[\"first_clean\"]), to_csr(vec_first, val_df[\"first_clean\"]), to_csr(vec_first, test_df[\"first_clean\"])\n","Xtr_last,  Xva_last,  Xte_last  = to_csr(vec_last,  train_df[\"last_clean\"]),  to_csr(vec_last,  val_df[\"last_clean\"]),  to_csr(vec_last,  test_df[\"last_clean\"])\n","\n","print({\n","    \"vocab_sizes\": {\n","        \"full\":  len(vec_full.vocabulary_),\n","        \"first\": len(vec_first.vocabulary_),\n","        \"last\":  len(vec_last.vocabulary_),\n","    },\n","    \"train_shapes\": {\n","        \"full\":  Xtr_full.shape,\n","        \"first\": Xtr_first.shape,\n","        \"last\":  Xtr_last.shape,\n","    }\n","})\n","\n","# -------------------\n","# Train models\n","# -------------------\n","lr_full  = train_logreg(Xtr_full,  y_tr)\n","lr_first = train_logreg(Xtr_first, y_tr)\n","lr_last  = train_logreg(Xtr_last,  y_tr)\n","\n","# Save models\n","joblib.dump(lr_full,  ART_DIR / \"lr_full.joblib\")\n","joblib.dump(lr_first, ART_DIR / \"lr_first.joblib\")\n","joblib.dump(lr_last,  ART_DIR / \"lr_last.joblib\")\n","\n","# -------------------\n","# Validation metrics\n","# -------------------\n","p_full_va  = lr_full.predict_proba(Xva_full)[:,1]\n","p_first_va = lr_first.predict_proba(Xva_first)[:,1]\n","p_last_va  = lr_last.predict_proba(Xva_last)[:,1]\n","\n","# Fusion: 1 - (1-p_first)*(1-p_last)\n","p_fusion_va = 1.0 - (1.0 - p_first_va) * (1.0 - p_last_va)\n","\n","metrics = {\n","    \"full\":   eval_probs(y_va, p_full_va,  label=\"logreg_full\"),\n","    \"first\":  eval_probs(y_va, p_first_va, label=\"logreg_first\"),\n","    \"last\":   eval_probs(y_va, p_last_va,  label=\"logreg_last\"),\n","    \"fusion\": eval_probs(y_va, p_fusion_va,label=\"fusion_first_last\"),\n","}\n","\n","print(json.dumps(metrics, indent=2))\n","\n","# Persist metrics for traceability\n","with open(ART_DIR / \"val_metrics.json\", \"w\") as f:\n","    json.dump(metrics, f, indent=2)\n","\n","# -------------------\n","# OPTIONAL: quick test-set snapshot at the chosen val thresholds\n","# We apply each model's \"best F1\" threshold from validation to the test set\n","# (This is only a sanity peek; the real selection happens after you review val metrics.)\n","# -------------------\n","def apply_threshold(y_true, p_scores, thr):\n","    y_hat = (p_scores >= thr).astype(int)\n","    cm = confusion_matrix(y_true, y_hat, labels=[0,1])\n","    return {\n","        \"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]),\n","        \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1])\n","    }\n","\n","# Compute test probs\n","p_full_te  = lr_full.predict_proba(Xte_full)[:,1]\n","p_first_te = lr_first.predict_proba(Xte_first)[:,1]\n","p_last_te  = lr_last.predict_proba(Xte_last)[:,1]\n","p_fusion_te= 1.0 - (1.0 - p_first_te) * (1.0 - p_last_te)\n","\n","test_snapshot = {}\n","for key, p in [(\"full\", p_full_te), (\"first\", p_first_te), (\"last\", p_last_te), (\"fusion\", p_fusion_te)]:\n","    thr = metrics[key][\"best_f1\"][\"threshold\"]\n","    test_snapshot[key] = {\n","        \"threshold_from_val_best_f1\": thr,\n","        \"confusion_matrix_on_test\": apply_threshold(y_te, p, thr)\n","    }\n","\n","with open(ART_DIR / \"test_snapshot.json\", \"w\") as f:\n","    json.dump(test_snapshot, f, indent=2)\n","\n","print(\"Saved artifacts to:\", str(ART_DIR.resolve()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEeAMfevDoXk","executionInfo":{"status":"ok","timestamp":1758480013493,"user_tz":240,"elapsed":28603,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"1cdfa3bc-457c-4333-dca1-23d141682111"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'vocab_sizes': {'full': 57322, 'first': 10835, 'last': 23277}, 'train_shapes': {'full': (18651, 57322), 'first': (18651, 10835), 'last': (18651, 23277)}}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["{\n","  \"full\": {\n","    \"label\": \"logreg_full\",\n","    \"pr_auc\": 0.8252703469956613,\n","    \"roc_auc\": 0.9651644116962037,\n","    \"best_f1\": {\n","      \"threshold\": 0.6819498025374165,\n","      \"precision\": 0.7647058823529411,\n","      \"recall\": 0.7857142857142857,\n","      \"f1\": 0.7750677506775069\n","    },\n","    \"p_at_recall_0.90\": {\n","      \"threshold\": 0.4506668394029149,\n","      \"precision\": 0.5578231292517006,\n","      \"recall\": 0.9010989010989011,\n","      \"f1\": 0.6890756302521008\n","    },\n","    \"r_at_precision_0.90\": {\n","      \"threshold\": 0.8530234181777896,\n","      \"precision\": 0.9004524886877828,\n","      \"recall\": 0.5467032967032966,\n","      \"f1\": 0.6803418803418803\n","    },\n","    \"confusion_matrix\": {\n","      \"tn\": 3545,\n","      \"fp\": 88,\n","      \"fn\": 78,\n","      \"tp\": 286\n","    },\n","    \"classification_report_macro_f1\": 0.8760950660774525,\n","    \"classification_report_SA_f1\": 0.7750677506775068\n","  },\n","  \"first\": {\n","    \"label\": \"logreg_first\",\n","    \"pr_auc\": 0.6816878126145355,\n","    \"roc_auc\": 0.915621984676485,\n","    \"best_f1\": {\n","      \"threshold\": 0.7091609112645911,\n","      \"precision\": 0.6505376344086021,\n","      \"recall\": 0.6648351648351648,\n","      \"f1\": 0.657608695652174\n","    },\n","    \"p_at_recall_0.90\": {\n","      \"threshold\": 0.2531675564350794,\n","      \"precision\": 0.2578616352201258,\n","      \"recall\": 0.9010989010989011,\n","      \"f1\": 0.4009779951100244\n","    },\n","    \"r_at_precision_0.90\": {\n","      \"threshold\": 0.9456756990445779,\n","      \"precision\": 0.9054054054054054,\n","      \"recall\": 0.18406593406593408,\n","      \"f1\": 0.3059360730593607\n","    },\n","    \"confusion_matrix\": {\n","      \"tn\": 3503,\n","      \"fp\": 130,\n","      \"fn\": 122,\n","      \"tp\": 242\n","    },\n","    \"classification_report_macro_f1\": 0.8114441935136042,\n","    \"classification_report_SA_f1\": 0.657608695652174\n","  },\n","  \"last\": {\n","    \"label\": \"logreg_last\",\n","    \"pr_auc\": 0.6864067173819369,\n","    \"roc_auc\": 0.9316733362976137,\n","    \"best_f1\": {\n","      \"threshold\": 0.7499912109239059,\n","      \"precision\": 0.7044776119402985,\n","      \"recall\": 0.6483516483516484,\n","      \"f1\": 0.675250357653791\n","    },\n","    \"p_at_recall_0.90\": {\n","      \"threshold\": 0.3435160373899307,\n","      \"precision\": 0.3492569002123142,\n","      \"recall\": 0.9038461538461539,\n","      \"f1\": 0.5038284839203676\n","    },\n","    \"r_at_precision_0.90\": {\n","      \"threshold\": 0.9758522863696467,\n","      \"precision\": 0.9230769230769231,\n","      \"recall\": 0.03296703296703297,\n","      \"f1\": 0.0636604774535809\n","    },\n","    \"confusion_matrix\": {\n","      \"tn\": 3534,\n","      \"fp\": 99,\n","      \"fn\": 128,\n","      \"tp\": 236\n","    },\n","    \"classification_report_macro_f1\": 0.8220665770448531,\n","    \"classification_report_SA_f1\": 0.6752503576537912\n","  },\n","  \"fusion\": {\n","    \"label\": \"fusion_first_last\",\n","    \"pr_auc\": 0.8346714088419733,\n","    \"roc_auc\": 0.9680069448855576,\n","    \"best_f1\": {\n","      \"threshold\": 0.9029604873573984,\n","      \"precision\": 0.7682291666666666,\n","      \"recall\": 0.8104395604395604,\n","      \"f1\": 0.7887700534759358\n","    },\n","    \"p_at_recall_0.90\": {\n","      \"threshold\": 0.7990109131550117,\n","      \"precision\": 0.6007326007326007,\n","      \"recall\": 0.9010989010989011,\n","      \"f1\": 0.7208791208791209\n","    },\n","    \"r_at_precision_0.90\": {\n","      \"threshold\": 0.9644121402567667,\n","      \"precision\": 0.9030837004405287,\n","      \"recall\": 0.5631868131868132,\n","      \"f1\": 0.6937394247038918\n","    },\n","    \"confusion_matrix\": {\n","      \"tn\": 3544,\n","      \"fp\": 89,\n","      \"fn\": 69,\n","      \"tp\": 295\n","    },\n","    \"classification_report_macro_f1\": 0.8834824598044875,\n","    \"classification_report_SA_f1\": 0.7887700534759359\n","  }\n","}\n","Saved artifacts to: /content/artifacts_baseline\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Step 5 — Calibration, threshold selection, and a production-style inference API"],"metadata":{"id":"zQIh_-KOGy3W"}},{"cell_type":"code","source":["# STEP 5: Calibrate (isotonic), pick thresholds, add inference function with fusion + abstain\n","\n","\"\"\"\n","* Fit isotonic calibration on validation scores (per view + fusion).\n","* Re-evaluate metrics on val after calibration; choose operating threshold:\n","* Evaluate on test at the chosen threshold.\n","* Save a lightweight inference module\n","\"\"\"\n","\n","from pathlib import Path\n","import json\n","import joblib\n","import re, unicodedata, json, joblib, numpy as np\n","import pandas as pd\n","import sqlite3, re, unicodedata\n","\n","from sklearn.isotonic import IsotonicRegression\n","from sklearn.metrics import (\n","    average_precision_score, roc_auc_score, precision_recall_curve, confusion_matrix, classification_report\n",")\n","\n","ART_IN   = Path(\"./artifacts_baseline\")   # from step 4\n","ART_OUT  = Path(\"./artifacts_calibrated\")\n","ART_OUT.mkdir(parents=True, exist_ok=True)\n","\n","# -------------------\n","# Load artifacts and data\n","# -------------------\n","vec_full  = joblib.load(ART_IN / \"vec_full_tfidf.joblib\")\n","vec_first = joblib.load(ART_IN / \"vec_first_tfidf.joblib\")\n","vec_last  = joblib.load(ART_IN / \"vec_last_tfidf.joblib\")\n","\n","lr_full   = joblib.load(ART_IN / \"lr_full.joblib\")\n","lr_first  = joblib.load(ART_IN / \"lr_first.joblib\")\n","lr_last   = joblib.load(ART_IN / \"lr_last.joblib\")\n","\n","# Fetch splits\n","con = sqlite3.connect(\"./sa_names.db\")\n","def fetch_split(split):\n","    q = \"\"\"\n","    SELECT name, name_ascii, first_clean, last_clean, is_south_asian\n","    FROM names_preprocessed\n","    WHERE name_ascii IN (SELECT name_ascii FROM names_splits WHERE split=?);\n","    \"\"\"\n","    return pd.read_sql_query(q, con, params=(split,))\n","val_df  = fetch_split(\"val\")\n","test_df = fetch_split(\"test\")\n","con.close()\n","\n","y_va = val_df[\"is_south_asian\"].to_numpy(dtype=int)\n","y_te = test_df[\"is_south_asian\"].to_numpy(dtype=int)\n","\n","# -------------------\n","# Get raw (uncalibrated) probabilities for VAL/TEST\n","# -------------------\n","def probs_for(df):\n","    Xf  = vec_full.transform(df[\"name_ascii\"].fillna(\"\"))\n","    Xfi = vec_first.transform(df[\"first_clean\"].fillna(\"\"))\n","    Xla = vec_last.transform(df[\"last_clean\"].fillna(\"\"))\n","\n","    p_full  = lr_full.predict_proba(Xf)[:,1]\n","    p_first = lr_first.predict_proba(Xfi)[:,1]\n","    p_last  = lr_last.predict_proba(Xla)[:,1]\n","    p_fusion = 1.0 - (1.0 - p_first) * (1.0 - p_last)\n","    return p_full, p_first, p_last, p_fusion\n","\n","p_full_va, p_first_va, p_last_va, p_fusion_va = probs_for(val_df)\n","p_full_te, p_first_te, p_last_te, p_fusion_te = probs_for(test_df)\n","\n","# -------------------\n","# Calibrate with isotonic regression on VAL\n","# -------------------\n","def fit_iso(y_true, p_scores):\n","    # Isotonic expects sorted by scores sometimes; but scikit handles internally.\n","    ir = IsotonicRegression(out_of_bounds=\"clip\")\n","    ir.fit(p_scores, y_true)\n","    return ir\n","\n","cal_full   = fit_iso(y_va, p_full_va)\n","cal_first  = fit_iso(y_va, p_first_va)\n","cal_last   = fit_iso(y_va, p_last_va)\n","cal_fusion = fit_iso(y_va, p_fusion_va)\n","\n","# Apply calibration\n","def apply_cal(ir, p): return np.clip(ir.predict(p), 0.0, 1.0)\n","\n","cp_full_va   = apply_cal(cal_full,   p_full_va)\n","cp_first_va  = apply_cal(cal_first,  p_first_va)\n","cp_last_va   = apply_cal(cal_last,   p_last_va)\n","cp_fusion_va = apply_cal(cal_fusion, p_fusion_va)\n","\n","cp_full_te   = apply_cal(cal_full,   p_full_te)\n","cp_first_te  = apply_cal(cal_first,  p_first_te)\n","cp_last_te   = apply_cal(cal_last,   p_last_te)\n","cp_fusion_te = apply_cal(cal_fusion, p_fusion_te)\n","\n","# -------------------\n","# Metric helper\n","# -------------------\n","def summarize(y_true, p, label):\n","    pr_auc = float(average_precision_score(y_true, p))\n","    try:\n","        roc = float(roc_auc_score(y_true, p))\n","    except ValueError:\n","        roc = float(\"nan\")\n","    precision, recall, thresholds = precision_recall_curve(y_true, p)\n","    thr = np.concatenate([thresholds, [1.0]])\n","    f1 = np.where((precision+recall)>0, 2*precision*recall/(precision+recall), 0.0)\n","    best_i = int(np.nanargmax(f1))\n","    best = {\"threshold\": float(thr[best_i]), \"precision\": float(precision[best_i]), \"recall\": float(recall[best_i]), \"f1\": float(f1[best_i])}\n","\n","    # P@R>=0.90 and R@P>=0.90\n","    def at_recall(target=0.90):\n","        ok = np.where(recall >= target)[0]\n","        if len(ok)==0: return None\n","        i = ok[np.argmax(precision[ok])]\n","        return {\"threshold\": float(thr[i]), \"precision\": float(precision[i]), \"recall\": float(recall[i]), \"f1\": float(f1[i])}\n","    def at_precision(target=0.90):\n","        ok = np.where(precision >= target)[0]\n","        if len(ok)==0: return None\n","        i = ok[np.argmax(recall[ok])]\n","        return {\"threshold\": float(thr[i]), \"precision\": float(precision[i]), \"recall\": float(recall[i]), \"f1\": float(f1[i])}\n","\n","    return {\n","        \"label\": label,\n","        \"pr_auc\": pr_auc,\n","        \"roc_auc\": roc,\n","        \"best_f1\": best,\n","        \"p_at_recall_0.90\": at_recall(0.90),\n","        \"r_at_precision_0.90\": at_precision(0.90),\n","    }\n","\n","metrics_val_cal = {\n","    \"full\":   summarize(y_va, cp_full_va,   \"cal_full\"),\n","    \"first\":  summarize(y_va, cp_first_va,  \"cal_first\"),\n","    \"last\":   summarize(y_va, cp_last_va,   \"cal_last\"),\n","    \"fusion\": summarize(y_va, cp_fusion_va, \"cal_fusion\"),\n","}\n","print(\"CALIBRATED VAL METRICS:\\n\", json.dumps(metrics_val_cal, indent=2))\n","\n","# -------------------\n","# Choose operating threshold from calibrated VAL (edit policy here)\n","# For directory completeness, prefer RECALL≥0.90 on fusion; fallback to best_F1 if not available.\n","# -------------------\n","op = metrics_val_cal[\"fusion\"][\"p_at_recall_0.90\"] or metrics_val_cal[\"fusion\"][\"best_f1\"]\n","OP_THRESHOLD = float(op[\"threshold\"])\n","print(\"Chosen operating threshold (fusion, calibrated):\", OP_THRESHOLD)\n","\n","# -------------------\n","# Test-set evaluation at the chosen threshold\n","# -------------------\n","def confusion(y_true, p, thr):\n","    y_hat = (p >= thr).astype(int)\n","    cm = confusion_matrix(y_true, y_hat, labels=[0,1])\n","    rpt = classification_report(y_true, y_hat, labels=[0,1], target_names=[\"non-SA\",\"SA\"], output_dict=True)\n","    return {\n","        \"threshold\": thr,\n","        \"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]),\n","        \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1]),\n","        \"sa_f1\": float(rpt[\"SA\"][\"f1-score\"]),\n","        \"macro_f1\": float(rpt[\"macro avg\"][\"f1-score\"])\n","    }\n","\n","test_eval = confusion(y_te, cp_fusion_te, OP_THRESHOLD)\n","print(\"TEST EVAL @ chosen threshold:\\n\", json.dumps(test_eval, indent=2))\n","\n","# -------------------\n","# Save calibrators + config\n","# -------------------\n","joblib.dump(cal_full,   ART_OUT / \"cal_full_isotonic.joblib\")\n","joblib.dump(cal_first,  ART_OUT / \"cal_first_isotonic.joblib\")\n","joblib.dump(cal_last,   ART_OUT / \"cal_last_isotonic.joblib\")\n","joblib.dump(cal_fusion, ART_OUT / \"cal_fusion_isotonic.joblib\")\n","\n","cfg = {\n","    \"threshold\": OP_THRESHOLD,\n","    \"policy\": \"p_at_recall_0.90 on calibrated fusion if available else best_f1\",\n","    \"abstain_band\": [0.45, 0.65]  # tweak after looking at calibration curves\n","}\n","with open(ART_OUT / \"inference_config.json\", \"w\") as f:\n","    json.dump(cfg, f, indent=2)\n","\n","print(\"Saved calibrated artifacts to:\", str(ART_OUT.resolve()))\n","\n","# -------------------\n","# Inference API (single-name)\n","# -------------------\n","# Same cleaning as training (minimal: lower, unicode norm, collapse spaces)\n","HONORIFICS = r\"\\b(dr|mr|mrs|ms|miss|mx|prof|sir|madam|lady|lord)\\b\\.?\"\n","SUFFIXES   = r\"\\b(jr|sr|ii|iii|iv|phd|md|esq)\\b\\.?\"\n","\n","def normalize_unicode(s: str) -> str:\n","    if not isinstance(s, str): return \"\"\n","    s = (s.replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\")\n","           .replace(\"\\u201C\",'\"').replace(\"\\u201D\",'\"')\n","           .replace(\"\\u2013\",\"-\").replace(\"\\u2014\",\"-\")\n","           .replace(\"\\u00A0\",\" \"))\n","    return unicodedata.normalize(\"NFKC\", s)\n","\n","def clean_name(raw: str) -> str:\n","    s = normalize_unicode(raw).strip().lower()\n","    s = re.sub(r\"[,\\u200b]\", \" \", s)\n","    s = re.sub(rf\"^\\s*{HONORIFICS}\\s+\", \"\", s)\n","    s = re.sub(rf\"\\s+{SUFFIXES}\\s*$\", \"\", s)\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","\n","def split_first_last(cleaned: str):\n","    if not cleaned: return \"\", \"\"\n","    parts = cleaned.split()\n","    if len(parts) == 1: return parts[0], \"\"\n","    return parts[0], parts[-1]\n","\n","# Load for inference\n","CAL_FULL   = joblib.load(ART_OUT / \"cal_full_isotonic.joblib\")\n","CAL_FIRST  = joblib.load(ART_OUT / \"cal_first_isotonic.joblib\")\n","CAL_LAST   = joblib.load(ART_OUT / \"cal_last_isotonic.joblib\")\n","CAL_FUSION = joblib.load(ART_OUT / \"cal_fusion_isotonic.joblib\")\n","CFG        = json.load(open(ART_OUT / \"inference_config.json\"))\n","THR        = CFG[\"threshold\"]\n","ABSTAIN_L, ABSTAIN_H = CFG[\"abstain_band\"]\n","\n","def predict_is_south_asian(name: str):\n","    name_c = clean_name(name)\n","    first, last = split_first_last(name_c)\n","\n","    Xf  = vec_full.transform([name_c])\n","    Xfi = vec_first.transform([first])\n","    Xla = vec_last.transform([last])\n","\n","    p_full   = lr_full.predict_proba(Xf)[:,1]\n","    p_first  = lr_first.predict_proba(Xfi)[:,1]\n","    p_last   = lr_last.predict_proba(Xla)[:,1]\n","    p_fusion = 1.0 - (1.0 - p_first) * (1.0 - p_last)\n","\n","    # Calibrated (index [0] to avoid NumPy deprecation)\n","    cp_full   = float(np.clip(CAL_FULL.predict(p_full)[0],    0, 1))\n","    cp_first  = float(np.clip(CAL_FIRST.predict(p_first)[0],  0, 1))\n","    cp_last   = float(np.clip(CAL_LAST.predict(p_last)[0],    0, 1))\n","    cp_fusion = float(np.clip(CAL_FUSION.predict(p_fusion)[0],0, 1))\n","\n","    # Decision with abstain band\n","    if cp_fusion < ABSTAIN_L:\n","        decision = \"non_sa\"\n","    elif cp_fusion > ABSTAIN_H:\n","        decision = \"sa\"\n","    else:\n","        decision = \"abstain\"\n","\n","    hard = \"sa\" if cp_fusion >= THR else \"non_sa\"\n","\n","    return {\n","        \"input\": name,\n","        \"clean\": name_c,\n","        \"first\": first,\n","        \"last\": last,\n","        \"probabilities\": {\n","            \"full\": cp_full,\n","            \"first\": cp_first,\n","            \"last\": cp_last,\n","            \"fusion\": cp_fusion\n","        },\n","        \"decision_abstain_band\": decision,\n","        \"hard_decision_at_threshold\": {\"threshold\": THR, \"label\": hard}\n","    }\n","\n","# Quick smoke test\n","for ex in [\"A. R. Rahman\", \"Daniel Singh\", \"Mary Thomas\", \"Noah Patel\", \"Kevin Johnson\"]:\n","    print(ex, \"->\", predict_is_south_asian(ex))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0DmcFF5vGMC_","executionInfo":{"status":"ok","timestamp":1758480014404,"user_tz":240,"elapsed":909,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"ab59e46a-f4fb-4d6b-bf67-276b89cc5655"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["CALIBRATED VAL METRICS:\n"," {\n","  \"full\": {\n","    \"label\": \"cal_full\",\n","    \"pr_auc\": 0.8186329985855617,\n","    \"roc_auc\": 0.9686349639900425,\n","    \"best_f1\": {\n","      \"threshold\": 0.45454545454545453,\n","      \"precision\": 0.7647058823529411,\n","      \"recall\": 0.7857142857142857,\n","      \"f1\": 0.7750677506775069\n","    },\n","    \"p_at_recall_0.90\": {\n","      \"threshold\": 0.1357142857142857,\n","      \"precision\": 0.554806070826307,\n","      \"recall\": 0.9038461538461539,\n","      \"f1\": 0.6875653082549633\n","    },\n","    \"r_at_precision_0.90\": {\n","      \"threshold\": 0.8055555555555556,\n","      \"precision\": 0.9036697247706422,\n","      \"recall\": 0.5412087912087912,\n","      \"f1\": 0.6769759450171821\n","    }\n","  },\n","  \"first\": {\n","    \"label\": \"cal_first\",\n","    \"pr_auc\": 0.6704845171367658,\n","    \"roc_auc\": 0.9203020692492204,\n","    \"best_f1\": {\n","      \"threshold\": 0.4318181818181818,\n","      \"precision\": 0.6505376344086021,\n","      \"recall\": 0.6648351648351648,\n","      \"f1\": 0.657608695652174\n","    },\n","    \"p_at_recall_0.90\": {\n","      \"threshold\": 0.02947845804988662,\n","      \"precision\": 0.2564302416212003,\n","      \"recall\": 0.9038461538461539,\n","      \"f1\": 0.39951426836672743\n","    },\n","    \"r_at_precision_0.90\": {\n","      \"threshold\": 0.9090909090909091,\n","      \"precision\": 0.9242424242424242,\n","      \"recall\": 0.16758241758241757,\n","      \"f1\": 0.2837209302325581\n","    }\n","  },\n","  \"last\": {\n","    \"label\": \"cal_last\",\n","    \"pr_auc\": 0.6827259905092183,\n","    \"roc_auc\": 0.9360774100658493,\n","    \"best_f1\": {\n","      \"threshold\": 0.4411764705882353,\n","      \"precision\": 0.7044776119402985,\n","      \"recall\": 0.6483516483516484,\n","      \"f1\": 0.675250357653791\n","    },\n","    \"p_at_recall_0.90\": {\n","      \"threshold\": 0.0625,\n","      \"precision\": 0.34407484407484407,\n","      \"recall\": 0.9093406593406593,\n","      \"f1\": 0.4992458521870286\n","    },\n","    \"r_at_precision_0.90\": {\n","      \"threshold\": 0.9,\n","      \"precision\": 0.9230769230769231,\n","      \"recall\": 0.03296703296703297,\n","      \"f1\": 0.0636604774535809\n","    }\n","  },\n","  \"fusion\": {\n","    \"label\": \"cal_fusion\",\n","    \"pr_auc\": 0.832362337844544,\n","    \"roc_auc\": 0.9713425165530865,\n","    \"best_f1\": {\n","      \"threshold\": 0.40816326530612246,\n","      \"precision\": 0.7682291666666666,\n","      \"recall\": 0.8104395604395604,\n","      \"f1\": 0.7887700534759358\n","    },\n","    \"p_at_recall_0.90\": {\n","      \"threshold\": 0.17045454545454544,\n","      \"precision\": 0.5935828877005348,\n","      \"recall\": 0.9148351648351648,\n","      \"f1\": 0.72\n","    },\n","    \"r_at_precision_0.90\": {\n","      \"threshold\": 0.8055555555555556,\n","      \"precision\": 0.9030837004405287,\n","      \"recall\": 0.5631868131868132,\n","      \"f1\": 0.6937394247038918\n","    }\n","  }\n","}\n","Chosen operating threshold (fusion, calibrated): 0.17045454545454544\n","TEST EVAL @ chosen threshold:\n"," {\n","  \"threshold\": 0.17045454545454544,\n","  \"tn\": 3419,\n","  \"fp\": 215,\n","  \"fn\": 34,\n","  \"tp\": 330,\n","  \"sa_f1\": 0.7260726072607261,\n","  \"macro_f1\": 0.8454689267431047\n","}\n","Saved calibrated artifacts to: /content/artifacts_calibrated\n","A. R. Rahman -> {'input': 'A. R. Rahman', 'clean': 'a. r. rahman', 'first': 'a.', 'last': 'rahman', 'probabilities': {'full': 0.375, 'first': 0.02947845804988662, 'last': 0.71875, 'fusion': 0.3157894736842105}, 'decision_abstain_band': 'non_sa', 'hard_decision_at_threshold': {'threshold': 0.17045454545454544, 'label': 'sa'}}\n","Daniel Singh -> {'input': 'Daniel Singh', 'clean': 'daniel singh', 'first': 'daniel', 'last': 'singh', 'probabilities': {'full': 0.16666666666666666, 'first': 0.0078125, 'last': 0.8571428571428571, 'fusion': 0.6491228070175439}, 'decision_abstain_band': 'abstain', 'hard_decision_at_threshold': {'threshold': 0.17045454545454544, 'label': 'sa'}}\n","Mary Thomas -> {'input': 'Mary Thomas', 'clean': 'mary thomas', 'first': 'mary', 'last': 'thomas', 'probabilities': {'full': 0.004310344827586207, 'first': 0.02821316614420063, 'last': 0.06521739130434782, 'fusion': 0.018315018315018316}, 'decision_abstain_band': 'non_sa', 'hard_decision_at_threshold': {'threshold': 0.17045454545454544, 'label': 'non_sa'}}\n","Noah Patel -> {'input': 'Noah Patel', 'clean': 'noah patel', 'first': 'noah', 'last': 'patel', 'probabilities': {'full': 0.5238095238095238, 'first': 0.017441860465116275, 'last': 0.8636363636363636, 'fusion': 0.6491228070175439}, 'decision_abstain_band': 'abstain', 'hard_decision_at_threshold': {'threshold': 0.17045454545454544, 'label': 'sa'}}\n","Kevin Johnson -> {'input': 'Kevin Johnson', 'clean': 'kevin johnson', 'first': 'kevin', 'last': 'johnson', 'probabilities': {'full': 0.0019907100199071004, 'first': 0.009445100354191263, 'last': 0.004549590536851683, 'fusion': 0.0026109660574412533}, 'decision_abstain_band': 'non_sa', 'hard_decision_at_threshold': {'threshold': 0.17045454545454544, 'label': 'non_sa'}}\n"]}]},{"cell_type":"markdown","source":["Step 5a: Precision-first threshold"],"metadata":{"id":"HASptiqRKREn"}},{"cell_type":"code","source":["# STEP 5a: Use calibrated fusion threshold at R@P>=0.90, update config & re-eval test\n","import json, joblib, numpy as np, sqlite3, pandas as pd\n","from sklearn.metrics import precision_recall_curve, confusion_matrix, classification_report\n","\n","ART_IN  = \"./artifacts_calibrated\"   # from Step 5\n","ART_BL  = \"./artifacts_baseline\"     # from Step 4\n","\n","# Load calibrator (fusion), models, vectorizers\n","cal_fusion = joblib.load(f\"{ART_IN}/cal_fusion_isotonic.joblib\")\n","vec_full   = joblib.load(f\"{ART_BL}/vec_full_tfidf.joblib\")\n","vec_first  = joblib.load(f\"{ART_BL}/vec_first_tfidf.joblib\")\n","vec_last   = joblib.load(f\"{ART_BL}/vec_last_tfidf.joblib\")\n","lr_full    = joblib.load(f\"{ART_BL}/lr_full.joblib\")\n","lr_first   = joblib.load(f\"{ART_BL}/lr_first.joblib\")\n","lr_last    = joblib.load(f\"{ART_BL}/lr_last.joblib\")\n","\n","def fetch(split: str):\n","    con = sqlite3.connect(\"./sa_names.db\")\n","    q = \"\"\"\n","    SELECT name_ascii, first_clean, last_clean, is_south_asian\n","    FROM names_preprocessed\n","    WHERE name_ascii IN (SELECT name_ascii FROM names_splits WHERE split=?);\n","    \"\"\"\n","    df = pd.read_sql_query(q, con, params=(split,))\n","    con.close()\n","    return df\n","\n","val_df  = fetch(\"val\")\n","test_df = fetch(\"test\")\n","y_va = val_df[\"is_south_asian\"].to_numpy(int)\n","y_te = test_df[\"is_south_asian\"].to_numpy(int)\n","\n","def calibrated_fusion_probs(df: pd.DataFrame) -> np.ndarray:\n","    Xf  = vec_full.transform(df[\"name_ascii\"].fillna(\"\"))\n","    Xfi = vec_first.transform(df[\"first_clean\"].fillna(\"\"))\n","    Xla = vec_last.transform(df[\"last_clean\"].fillna(\"\"))\n","\n","    p_first = lr_first.predict_proba(Xfi)[:,1]\n","    p_last  = lr_last.predict_proba(Xla)[:,1]\n","    p_fus   = 1.0 - (1.0 - p_first) * (1.0 - p_last)\n","    cp_fus  = np.clip(cal_fusion.predict(p_fus), 0, 1)\n","    return cp_fus\n","\n","cp_fus_va = calibrated_fusion_probs(val_df)\n","cp_fus_te = calibrated_fusion_probs(test_df)\n","\n","# Choose threshold achieving R@P >= 0.90 on VAL; fallback to best-F1 if none\n","prec, rec, thr = precision_recall_curve(y_va, cp_fus_va)\n","thr = np.concatenate([thr, [1.0]])\n","mask = np.where(prec >= 0.90)[0]\n","if len(mask) == 0:\n","    # fallback: best F1\n","    f1 = np.where((prec + rec) > 0, 2 * prec * rec / (prec + rec), 0.0)\n","    i = int(np.argmax(f1))\n","    thr_star = float(thr[i])\n","    policy = \"best_f1_fallback\"\n","else:\n","    i = mask[np.argmax(rec[mask])]  # within P>=0.90, take highest recall\n","    thr_star = float(thr[i])\n","    policy = \"r_at_precision_0.90\"\n","\n","# Evaluate on TEST at the chosen threshold\n","y_hat = (cp_fus_te >= thr_star).astype(int)\n","cm = confusion_matrix(y_te, y_hat, labels=[0,1])\n","rpt = classification_report(y_te, y_hat, labels=[0,1], target_names=[\"non-SA\",\"SA\"], output_dict=True)\n","\n","report = {\n","    \"policy\": policy,\n","    \"threshold\": thr_star,\n","    \"test_confusion\": {\"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]), \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1])},\n","    \"test_sa_f1\": float(rpt[\"SA\"][\"f1-score\"]),\n","    \"test_macro_f1\": float(rpt[\"macro avg\"][\"f1-score\"]),\n","}\n","print(json.dumps(report, indent=2))\n","\n","# Update inference_config.json with new threshold and tighter abstain band\n","cfg_path = f\"{ART_IN}/inference_config.json\"\n","cfg = json.load(open(cfg_path))\n","cfg[\"threshold\"] = thr_star\n","cfg[\"policy\"] = \"fusion_calibrated_r_at_precision_0.90\"\n","cfg[\"abstain_band\"] = [max(0.0, thr_star - 0.10), min(1.0, thr_star + 0.10)]\n","json.dump(cfg, open(cfg_path, \"w\"), indent=2)\n","print(\"Updated config:\", cfg_path, \"->\", cfg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3ZLcWqZKRcV","executionInfo":{"status":"ok","timestamp":1758480015280,"user_tz":240,"elapsed":874,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"20c6da88-d1b7-4eb6-d83e-87f8645b008b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"policy\": \"r_at_precision_0.90\",\n","  \"threshold\": 0.8055555555555556,\n","  \"test_confusion\": {\n","    \"tn\": 3613,\n","    \"fp\": 21,\n","    \"fn\": 146,\n","    \"tp\": 218\n","  },\n","  \"test_sa_f1\": 0.7230514096185738,\n","  \"test_macro_f1\": 0.8502312370695331\n","}\n","Updated config: ./artifacts_calibrated/inference_config.json -> {'threshold': 0.8055555555555556, 'policy': 'fusion_calibrated_r_at_precision_0.90', 'abstain_band': [0.7055555555555556, 0.9055555555555556]}\n"]}]},{"cell_type":"markdown","source":["Step 6: Baseline tightening"],"metadata":{"id":"JaeHmWtSK3gL"}},{"cell_type":"code","source":["# Step 6: SGDClassifier (logistic) sweep + early stopping + calibration + precision-first operating point\n","\n","from pathlib import Path\n","import json, sqlite3, joblib, numpy as np, pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import (\n","    average_precision_score, roc_auc_score, precision_recall_curve,\n","    classification_report, confusion_matrix\n",")\n","from sklearn.isotonic import IsotonicRegression\n","from sklearn.utils.class_weight import compute_class_weight\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","DB_PATH = \"./sa_names.db\"\n","ART_OUT = Path(\"./artifacts_tuned_sgd\")\n","ART_OUT.mkdir(parents=True, exist_ok=True)\n","SEED = 42\n","rng = np.random.default_rng(SEED)\n","\n","# ----------------- data helpers -----------------\n","def fetch_split(split: str, cols=('name_ascii','first_clean','last_clean','is_south_asian')):\n","    con = sqlite3.connect(DB_PATH)\n","    q = f\"\"\"\n","    SELECT {\",\".join(cols)}\n","    FROM names_preprocessed\n","    WHERE name_ascii IN (SELECT name_ascii FROM names_splits WHERE split=?);\n","    \"\"\"\n","    df = pd.read_sql_query(q, con, params=(split,))\n","    con.close()\n","    return df\n","\n","train_df = fetch_split(\"train\")\n","val_df   = fetch_split(\"val\")\n","test_df  = fetch_split(\"test\")\n","\n","y_tr = train_df[\"is_south_asian\"].to_numpy(int)\n","y_va = val_df[\"is_south_asian\"].to_numpy(int)\n","y_te = test_df[\"is_south_asian\"].to_numpy(int)\n","\n","# sample weights for imbalance (like class_weight='balanced')\n","classes = np.array([0,1], dtype=int)\n","cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n","w_map = {0: cw[0], 1: cw[1]}\n","sw_tr = np.array([w_map[y] for y in y_tr], dtype=float)\n","\n","# ----------------- vectorizers (fit on train only) -----------------\n","def fit_vec(series, ngram=(2,5), min_df=2):\n","    return TfidfVectorizer(analyzer=\"char\", ngram_range=ngram, min_df=min_df, lowercase=False, norm=\"l2\").fit(series.fillna(\"\"))\n","\n","vecs = {\n","    \"full\":  fit_vec(train_df[\"name_ascii\"]),\n","    \"first\": fit_vec(train_df[\"first_clean\"]),\n","    \"last\":  fit_vec(train_df[\"last_clean\"]),\n","}\n","def X_of(vec, series): return vec.transform(series.fillna(\"\"))\n","\n","Xtr = {\n","    \"full\":  X_of(vecs[\"full\"],  train_df[\"name_ascii\"]),\n","    \"first\": X_of(vecs[\"first\"], train_df[\"first_clean\"]),\n","    \"last\":  X_of(vecs[\"last\"],  train_df[\"last_clean\"]),\n","}\n","Xva = {\n","    \"full\":  X_of(vecs[\"full\"],  val_df[\"name_ascii\"]),\n","    \"first\": X_of(vecs[\"first\"], val_df[\"first_clean\"]),\n","    \"last\":  X_of(vecs[\"last\"],  val_df[\"last_clean\"]),\n","}\n","Xte = {\n","    \"full\":  X_of(vecs[\"full\"],  test_df[\"name_ascii\"]),\n","    \"first\": X_of(vecs[\"first\"], test_df[\"first_clean\"]),\n","    \"last\":  X_of(vecs[\"last\"],  test_df[\"last_clean\"]),\n","}\n","\n","# ----------------- fast sweep with SGDClassifier -----------------\n","grid_alpha = [1e-5, 3e-5, 1e-4, 3e-4, 1e-3]\n","grid_l1    = [0.0, 0.3, 0.5]   # 0.0 ~ pure L2\n","\n","def train_sgd(Xtr, ytr, Xva, yva, alpha, l1_ratio):\n","    # Early stopping uses Xva/yva internally\n","    clf = SGDClassifier(\n","        loss=\"log_loss\",\n","        penalty=\"elasticnet\" if l1_ratio>0 else \"l2\",\n","        l1_ratio=l1_ratio if l1_ratio>0 else 0.0,\n","        alpha=alpha,\n","        max_iter=2000,\n","        tol=1e-4,\n","        early_stopping=True,\n","        n_iter_no_change=5,\n","        validation_fraction=0.1,   # still keeps our separate val for model selection; early stopping uses internal split of train\n","        random_state=SEED,\n","        n_jobs=-1\n","    )\n","    clf.fit(Xtr, ytr, sample_weight=sw_tr)  # handle imbalance\n","    # Evaluate on our external validation\n","    p_va = clf.predict_proba(Xva)[:,1]\n","    pr = float(average_precision_score(yva, p_va))\n","    try:\n","        roc = float(roc_auc_score(yva, p_va))\n","    except ValueError:\n","        roc = float(\"nan\")\n","    prec, rec, thr = precision_recall_curve(yva, p_va)\n","    thr = np.concatenate([thr, [1.0]])\n","    f1 = np.where((prec+rec)>0, 2*prec*rec/(prec+rec), 0.0)\n","    best_f1 = float(np.nanmax(f1))\n","    return clf, p_va, {\"pr_auc\": pr, \"roc_auc\": roc, \"best_f1\": best_f1}\n","\n","results = {\"full\": [], \"first\": [], \"last\": []}\n","val_scores_cache = {\"full\": None, \"first\": None, \"last\": None}\n","\n","for view in [\"full\",\"first\",\"last\"]:\n","    for alpha in grid_alpha:\n","        for l1 in grid_l1:\n","            clf, pva, scores = train_sgd(Xtr[view], y_tr, Xva[view], y_va, alpha, l1)\n","            results[view].append({\n","                \"view\": view, \"alpha\": alpha, \"l1_ratio\": l1,\n","                \"pr_auc\": scores[\"pr_auc\"], \"roc_auc\": scores[\"roc_auc\"], \"best_f1\": scores[\"best_f1\"],\n","                \"model\": clf\n","            })\n","\n","# pick best per view by PR-AUC, tie-break by best_F1\n","best = {}\n","for view in [\"full\",\"first\",\"last\"]:\n","    cand = sorted(results[view], key=lambda d: (d[\"pr_auc\"], d[\"best_f1\"]), reverse=True)[0]\n","    best[view] = cand\n","    print(f\"Best {view}: alpha={cand['alpha']} l1={cand['l1_ratio']} | PR-AUC={cand['pr_auc']:.3f} F1*={cand['best_f1']:.3f}\")\n","\n","# ----------------- calibrate best models (isotonic on external VAL) -----------------\n","def fit_iso(y, p):\n","    ir = IsotonicRegression(out_of_bounds=\"clip\")\n","    ir.fit(p, y)\n","    return ir\n","\n","cal = {}\n","for view in [\"full\",\"first\",\"last\"]:\n","    model = best[view][\"model\"]\n","    p_va = model.predict_proba(Xva[view])[:,1]\n","    cal[view] = fit_iso(y_va, p_va)\n","\n","def calibrated_probs(view, split):\n","    if split==\"val\":\n","        X = Xva[view]\n","    elif split==\"test\":\n","        X = Xte[view]\n","    else:\n","        raise ValueError\n","    model = best[view][\"model\"]\n","    raw = model.predict_proba(X)[:,1]\n","    return np.clip(cal[view].predict(raw), 0, 1)\n","\n","cp_first_va = calibrated_probs(\"first\",\"val\")\n","cp_last_va  = calibrated_probs(\"last\",\"val\")\n","cp_fusion_va = 1.0 - (1.0 - cp_first_va) * (1.0 - cp_last_va)\n","\n","# precision-first threshold on VAL (R@P>=0.90), fallback to best F1\n","prec, rec, thr = precision_recall_curve(y_va, cp_fusion_va)\n","thr = np.concatenate([thr, [1.0]])\n","mask = np.where(prec >= 0.90)[0]\n","if len(mask)==0:\n","    f1 = np.where((prec+rec)>0, 2*prec*rec/(prec+rec), 0.0)\n","    i = int(np.argmax(f1))\n","    thr_star = float(thr[i]); policy = \"best_f1_fallback\"\n","else:\n","    i = mask[np.argmax(rec[mask])]\n","    thr_star = float(thr[i]); policy = \"r_at_precision_0.90\"\n","print(\"Chosen policy:\", policy, \"| threshold:\", thr_star)\n","\n","# evaluate on TEST\n","cp_first_te = calibrated_probs(\"first\",\"test\")\n","cp_last_te  = calibrated_probs(\"last\",\"test\")\n","cp_fusion_te = 1.0 - (1.0 - cp_first_te) * (1.0 - cp_last_te)\n","\n","y_hat = (cp_fusion_te >= thr_star).astype(int)\n","cm = confusion_matrix(y_te, y_hat, labels=[0,1])\n","rpt = classification_report(y_te, y_hat, labels=[0,1], target_names=[\"non-SA\",\"SA\"], output_dict=True)\n","\n","summary = {\n","    \"test_confusion\": {\"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]), \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1])},\n","    \"test_SA_f1\": float(rpt[\"SA\"][\"f1-score\"]),\n","    \"test_macro_f1\": float(rpt[\"macro avg\"][\"f1-score\"]),\n","}\n","print(json.dumps(summary, indent=2))\n","\n","# ----------------- save tuned artifacts -----------------\n","# vectorizers\n","joblib.dump(vecs[\"full\"],  ART_OUT / \"vec_full_tfidf.joblib\")\n","joblib.dump(vecs[\"first\"], ART_OUT / \"vec_first_tfidf.joblib\")\n","joblib.dump(vecs[\"last\"],  ART_OUT / \"vec_last_tfidf.joblib\")\n","\n","# models\n","joblib.dump(best[\"full\"][\"model\"],  ART_OUT / \"sgd_full_tuned.joblib\")\n","joblib.dump(best[\"first\"][\"model\"], ART_OUT / \"sgd_first_tuned.joblib\")\n","joblib.dump(best[\"last\"][\"model\"],  ART_OUT / \"sgd_last_tuned.joblib\")\n","\n","# calibrators\n","joblib.dump(cal[\"full\"],  ART_OUT / \"cal_full_isotonic.joblib\")\n","joblib.dump(cal[\"first\"], ART_OUT / \"cal_first_isotonic.joblib\")\n","joblib.dump(cal[\"last\"],  ART_OUT / \"cal_last_isotonic.joblib\")\n","\n","# config\n","cfg = {\n","    \"threshold\": thr_star,\n","    \"policy\": policy,\n","    \"abstain_band\": [max(0.0, thr_star-0.10), min(1.0, thr_star+0.10)],\n","    \"best_params\": {\n","        \"full\":  {\"alpha\": float(best[\"full\"][\"alpha\"]),  \"l1_ratio\": float(best[\"full\"][\"l1_ratio\"])},\n","        \"first\": {\"alpha\": float(best[\"first\"][\"alpha\"]), \"l1_ratio\": float(best[\"first\"][\"l1_ratio\"])},\n","        \"last\":  {\"alpha\": float(best[\"last\"][\"alpha\"]),  \"l1_ratio\": float(best[\"last\"][\"l1_ratio\"])},\n","    }\n","}\n","json.dump(cfg, open(ART_OUT / \"inference_config.json\",\"w\"), indent=2)\n","print(\"Saved tuned artifacts to:\", str(ART_OUT.resolve()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17VjthToK3xo","executionInfo":{"status":"ok","timestamp":1758480025444,"user_tz":240,"elapsed":10166,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"1830b285-2a13-4f82-df18-fc2da949c510"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4165493766.py:105: RuntimeWarning: invalid value encountered in divide\n","  f1 = np.where((prec+rec)>0, 2*prec*rec/(prec+rec), 0.0)\n","/tmp/ipython-input-4165493766.py:105: RuntimeWarning: invalid value encountered in divide\n","  f1 = np.where((prec+rec)>0, 2*prec*rec/(prec+rec), 0.0)\n"]},{"output_type":"stream","name":"stdout","text":["Best full: alpha=1e-05 l1=0.0 | PR-AUC=0.851 F1*=0.798\n","Best first: alpha=1e-05 l1=0.0 | PR-AUC=0.688 F1*=0.663\n","Best last: alpha=1e-05 l1=0.0 | PR-AUC=0.728 F1*=0.718\n","Chosen policy: r_at_precision_0.90 | threshold: 0.8443938901527462\n","{\n","  \"test_confusion\": {\n","    \"tn\": 3597,\n","    \"fp\": 37,\n","    \"fn\": 123,\n","    \"tp\": 241\n","  },\n","  \"test_SA_f1\": 0.7507788161993769,\n","  \"test_macro_f1\": 0.8645109745941133\n","}\n","Saved tuned artifacts to: /content/artifacts_tuned_sgd\n"]}]},{"cell_type":"markdown","source":["# Phase X: Creating Primary"],"metadata":{"id":"HTlcQcozQYLH"}},{"cell_type":"code","source":["# STEP 7: Char-BiLSTM (two-tower: first + last), calibrated, precision-first threshold\n","\n","\"\"\"\n","* Build a character vocab from train names.\n","* Encode first and last separately with shared Embedding + BiLSTM (two inputs).\n","* Concatenate towers → dense → sigmoid.\n","* Train with class weights, early stopping, ReduceLROnPlateau.\n","* Calibrate with isotonic on val.\n","* Choose precision-first threshold (R@P≥0.90) on the calibrated fusion (here the model already learns fusion, but we’ll still report).\n","* Evaluate on test and save artifacts.\n","\"\"\"\n","\n","import sqlite3, json, re, unicodedata, numpy as np, pandas as pd, joblib, os\n","from pathlib import Path\n","import tensorflow as tf\n","from sklearn.isotonic import IsotonicRegression\n","from sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix, average_precision_score, roc_auc_score\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","\n","DB_PATH = \"./sa_names.db\"\n","ART_DIR = Path(\"./artifacts_char_bilstm\")\n","ART_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# ---------- 1) Load splits ----------\n","def fetch_split(split: str):\n","    con = sqlite3.connect(DB_PATH)\n","    q = \"\"\"\n","    SELECT first_clean, last_clean, is_south_asian\n","    FROM names_preprocessed\n","    WHERE name_ascii IN (SELECT name_ascii FROM names_splits WHERE split=?);\n","    \"\"\"\n","    df = pd.read_sql_query(q, con, params=(split,))\n","    con.close()\n","    return df\n","\n","train_df = fetch_split(\"train\")\n","val_df   = fetch_split(\"val\")\n","test_df  = fetch_split(\"test\")\n","\n","y_tr = train_df[\"is_south_asian\"].to_numpy(np.int32)\n","y_va = val_df[\"is_south_asian\"].to_numpy(np.int32)\n","y_te = test_df[\"is_south_asian\"].to_numpy(np.int32)\n","\n","# ---------- 2) Char vocab & encoding ----------\n","def build_char_vocab(series_list):\n","    chars = set()\n","    for s in series_list:\n","        for name in s.fillna(\"\"):\n","            chars.update(list(name))\n","    # reserve 0 for PAD, 1 for OOV\n","    itos = [\"<PAD>\", \"<OOV>\"] + sorted(chars)\n","    stoi = {ch:i for i,ch in enumerate(itos)}\n","    return stoi, itos\n","\n","stoi, itos = build_char_vocab([\n","    train_df[\"first_clean\"], train_df[\"last_clean\"]\n","])\n","\n","MAX_LEN_FIRST = 20\n","MAX_LEN_LAST  = 25\n","\n","def encode(name: str, max_len: int):\n","    idxs = []\n","    for ch in (name or \"\")[:max_len]:\n","        idxs.append(stoi.get(ch, 1))  # 1 = OOV\n","    if len(idxs) < max_len:\n","        idxs += [0]*(max_len-len(idxs))  # 0 = PAD\n","    return np.array(idxs, dtype=np.int32)\n","\n","def encode_df(df: pd.DataFrame):\n","    X_first = np.stack([encode(x, MAX_LEN_FIRST) for x in df[\"first_clean\"].fillna(\"\")], axis=0)\n","    X_last  = np.stack([encode(x, MAX_LEN_LAST)  for x in df[\"last_clean\"].fillna(\"\")], axis=0)\n","    return X_first, X_last\n","\n","Xtr_first, Xtr_last = encode_df(train_df)\n","Xva_first, Xva_last = encode_df(val_df)\n","Xte_first, Xte_last = encode_df(test_df)\n","\n","# ---------- 3) Model (shared embedding + BiLSTM per tower) ----------\n","VOCAB_SIZE = len(itos)\n","EMB_DIM = 32\n","LSTM_UNITS = 64\n","DROPOUT = 0.25\n","\n","first_in = tf.keras.Input(shape=(MAX_LEN_FIRST,), name=\"first\")\n","last_in  = tf.keras.Input(shape=(MAX_LEN_LAST,),  name=\"last\")\n","\n","shared_emb = tf.keras.layers.Embedding(\n","    input_dim=VOCAB_SIZE, output_dim=EMB_DIM, mask_zero=True, name=\"char_emb\"\n",")\n","\n","def tower(x, name_prefix):\n","    x = shared_emb(x)\n","    x = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=False, dropout=DROPOUT, recurrent_dropout=0.0, use_cudnn=False),\n","        name=f\"{name_prefix}_bilstm\")(x)\n","    x = tf.keras.layers.Dense(64, activation=\"relu\", name=f\"{name_prefix}_dense\")(x)\n","    x = tf.keras.layers.Dropout(DROPOUT, name=f\"{name_prefix}_drop\")(x)\n","    return x\n","\n","t1 = tower(first_in, \"first\")\n","t2 = tower(last_in,  \"last\")\n","\n","h = tf.keras.layers.Concatenate(name=\"concat\")([t1, t2])\n","h = tf.keras.layers.Dense(64, activation=\"relu\")(h)\n","h = tf.keras.layers.Dropout(DROPOUT)(h)\n","out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"out\")(h)\n","\n","model = tf.keras.Model(inputs=[first_in, last_in], outputs=out)\n","model.summary()\n","\n","# ---------- 4) Train setup ----------\n","# Class imbalance → class weights\n","classes = np.array([0,1], dtype=int)\n","cw = compute_class_weight(\"balanced\", classes=classes, y=y_tr)\n","class_weight = {0: float(cw[0]), 1: float(cw[1])}\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-3),\n","    loss=\"binary_crossentropy\",\n","    metrics=[tf.keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\"), tf.keras.metrics.AUC(curve=\"ROC\", name=\"roc_auc\")]\n",")\n","\n","callbacks = [\n","    tf.keras.callbacks.EarlyStopping(monitor=\"val_pr_auc\", mode=\"max\", patience=5, restore_best_weights=True),\n","    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_pr_auc\", mode=\"max\", factor=0.5, patience=2, verbose=1, min_lr=1e-5)\n","]\n","\n","history = model.fit(\n","    x={\"first\": Xtr_first, \"last\": Xtr_last}, y=y_tr,\n","    validation_data=({\"first\": Xva_first, \"last\": Xva_last}, y_va),\n","    epochs=25,\n","    batch_size=256,\n","    class_weight=class_weight,\n","    callbacks=callbacks,\n","    verbose=1\n",")\n","\n","# ---------- 5) Probabilities, calibration, threshold selection ----------\n","p_va_raw = model.predict({\"first\": Xva_first, \"last\": Xva_last}, batch_size=512).reshape(-1)\n","p_te_raw = model.predict({\"first\": Xte_first, \"last\": Xte_last}, batch_size=512).reshape(-1)\n","\n","# Isotonic calibration on VAL\n","iso = IsotonicRegression(out_of_bounds=\"clip\")\n","iso.fit(p_va_raw, y_va)\n","\n","cp_va = np.clip(iso.predict(p_va_raw), 0, 1)\n","cp_te = np.clip(iso.predict(p_te_raw), 0, 1)\n","\n","# Precision-first threshold (R@P>=0.90) on VAL\n","prec, rec, thr = precision_recall_curve(y_va, cp_va)\n","thr = np.concatenate([thr, [1.0]])\n","mask = np.where(prec >= 0.90)[0]\n","if len(mask) == 0:\n","    # fallback to best-F1\n","    f1 = np.where((prec+rec)>0, 2*prec*rec/(prec+rec), 0.0)\n","    i = int(np.argmax(f1)); thr_star = float(thr[i]); policy = \"best_f1_fallback\"\n","else:\n","    i = mask[np.argmax(rec[mask])]\n","    thr_star = float(thr[i]); policy = \"r_at_precision_0.90\"\n","\n","# ---------- 6) Test evaluation ----------\n","y_hat = (cp_te >= thr_star).astype(int)\n","cm = confusion_matrix(y_te, y_hat, labels=[0,1])\n","rpt = classification_report(y_te, y_hat, labels=[0,1], target_names=[\"non-SA\",\"SA\"], output_dict=True)\n","\n","report = {\n","    \"policy\": policy,\n","    \"threshold\": thr_star,\n","    \"val_pr_auc\": float(average_precision_score(y_va, cp_va)),\n","    \"val_roc_auc\": float(roc_auc_score(y_va, cp_va)),\n","    \"test_confusion\": {\"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]), \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1])},\n","    \"test_SA_f1\": float(rpt[\"SA\"][\"f1-score\"]),\n","    \"test_macro_f1\": float(rpt[\"macro avg\"][\"f1-score\"]),\n","}\n","print(json.dumps(report, indent=2))\n","\n","# ---------- 7) Save artifacts ----------\n","model.save(ART_DIR / \"char_bilstm_twintower.keras\")\n","joblib.dump({\"stoi\": stoi, \"itos\": itos, \"MAX_LEN_FIRST\": MAX_LEN_FIRST, \"MAX_LEN_LAST\": MAX_LEN_LAST}, ART_DIR / \"char_vocab.joblib\")\n","joblib.dump(iso, ART_DIR / \"cal_isotonic.joblib\")\n","json.dump({\"threshold\": thr_star, \"policy\": policy, \"abstain_band\": [max(0.0, thr_star-0.10), min(1.0, thr_star+0.10)]}, open(ART_DIR / \"inference_config.json\",\"w\"), indent=2)\n","\n","print(\"Saved char-BiLSTM artifacts to:\", str(ART_DIR.resolve()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"HiALGEaYQc7N","executionInfo":{"status":"ok","timestamp":1758480958760,"user_tz":240,"elapsed":933319,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"3ec98e72-c134-46bb-fab7-afc2d6f0ee1a"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ first (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ last (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ char_emb            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m2,496\u001b[0m │ first[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n","│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │ last[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ first[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ last[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ first_bilstm        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m49,664\u001b[0m │ char_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n","│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ last_bilstm         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m49,664\u001b[0m │ char_emb[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n","│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ first_dense (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ first_bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ last_dense (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ last_bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ first_drop          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ first_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ last_drop (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ last_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concat              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ first_drop[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n","│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ last_drop[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ out (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ first (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ last (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ char_emb            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,496</span> │ first[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ last[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ first[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ last[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ first_bilstm        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span> │ char_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ last_bilstm         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span> │ char_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ first_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ first_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ last_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ last_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ first_drop          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ first_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ last_drop (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ last_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concat              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ first_drop[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ last_drop[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m126,657\u001b[0m (494.75 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,657</span> (494.75 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m126,657\u001b[0m (494.75 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,657</span> (494.75 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 302ms/step - loss: 0.5730 - pr_auc: 0.3386 - roc_auc: 0.7792 - val_loss: 0.3482 - val_pr_auc: 0.5690 - val_roc_auc: 0.9081 - learning_rate: 0.0030\n","Epoch 2/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 339ms/step - loss: 0.4124 - pr_auc: 0.5339 - roc_auc: 0.8959 - val_loss: 0.3692 - val_pr_auc: 0.6035 - val_roc_auc: 0.9189 - learning_rate: 0.0030\n","Epoch 3/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 303ms/step - loss: 0.3924 - pr_auc: 0.5825 - roc_auc: 0.9059 - val_loss: 0.4085 - val_pr_auc: 0.6256 - val_roc_auc: 0.9223 - learning_rate: 0.0030\n","Epoch 4/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 319ms/step - loss: 0.3651 - pr_auc: 0.6193 - roc_auc: 0.9189 - val_loss: 0.4188 - val_pr_auc: 0.6534 - val_roc_auc: 0.9266 - learning_rate: 0.0030\n","Epoch 5/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 334ms/step - loss: 0.3541 - pr_auc: 0.6419 - roc_auc: 0.9236 - val_loss: 0.3890 - val_pr_auc: 0.6668 - val_roc_auc: 0.9285 - learning_rate: 0.0030\n","Epoch 6/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 283ms/step - loss: 0.3392 - pr_auc: 0.6704 - roc_auc: 0.9301 - val_loss: 0.3720 - val_pr_auc: 0.6733 - val_roc_auc: 0.9306 - learning_rate: 0.0030\n","Epoch 7/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 290ms/step - loss: 0.3271 - pr_auc: 0.6899 - roc_auc: 0.9347 - val_loss: 0.3393 - val_pr_auc: 0.6921 - val_roc_auc: 0.9311 - learning_rate: 0.0030\n","Epoch 8/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 350ms/step - loss: 0.3098 - pr_auc: 0.7051 - roc_auc: 0.9418 - val_loss: 0.3389 - val_pr_auc: 0.7026 - val_roc_auc: 0.9334 - learning_rate: 0.0030\n","Epoch 9/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 306ms/step - loss: 0.3012 - pr_auc: 0.7274 - roc_auc: 0.9453 - val_loss: 0.3369 - val_pr_auc: 0.7105 - val_roc_auc: 0.9372 - learning_rate: 0.0030\n","Epoch 10/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 283ms/step - loss: 0.2826 - pr_auc: 0.7456 - roc_auc: 0.9517 - val_loss: 0.3180 - val_pr_auc: 0.7262 - val_roc_auc: 0.9408 - learning_rate: 0.0030\n","Epoch 11/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 282ms/step - loss: 0.2758 - pr_auc: 0.7653 - roc_auc: 0.9544 - val_loss: 0.3020 - val_pr_auc: 0.7327 - val_roc_auc: 0.9427 - learning_rate: 0.0030\n","Epoch 12/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 284ms/step - loss: 0.2589 - pr_auc: 0.7785 - roc_auc: 0.9597 - val_loss: 0.2867 - val_pr_auc: 0.7329 - val_roc_auc: 0.9426 - learning_rate: 0.0030\n","Epoch 13/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 324ms/step - loss: 0.2464 - pr_auc: 0.7874 - roc_auc: 0.9635 - val_loss: 0.2847 - val_pr_auc: 0.7354 - val_roc_auc: 0.9416 - learning_rate: 0.0030\n","Epoch 14/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 282ms/step - loss: 0.2311 - pr_auc: 0.7934 - roc_auc: 0.9674 - val_loss: 0.2675 - val_pr_auc: 0.7416 - val_roc_auc: 0.9391 - learning_rate: 0.0030\n","Epoch 15/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 284ms/step - loss: 0.2187 - pr_auc: 0.8244 - roc_auc: 0.9710 - val_loss: 0.2485 - val_pr_auc: 0.7470 - val_roc_auc: 0.9407 - learning_rate: 0.0030\n","Epoch 16/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 282ms/step - loss: 0.2079 - pr_auc: 0.8252 - roc_auc: 0.9737 - val_loss: 0.2434 - val_pr_auc: 0.7486 - val_roc_auc: 0.9389 - learning_rate: 0.0030\n","Epoch 17/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 322ms/step - loss: 0.1982 - pr_auc: 0.8329 - roc_auc: 0.9763 - val_loss: 0.2684 - val_pr_auc: 0.7474 - val_roc_auc: 0.9388 - learning_rate: 0.0030\n","Epoch 18/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 283ms/step - loss: 0.1785 - pr_auc: 0.8479 - roc_auc: 0.9799 - val_loss: 0.2245 - val_pr_auc: 0.7541 - val_roc_auc: 0.9352 - learning_rate: 0.0030\n","Epoch 19/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 291ms/step - loss: 0.1571 - pr_auc: 0.8692 - roc_auc: 0.9842 - val_loss: 0.2178 - val_pr_auc: 0.7665 - val_roc_auc: 0.9308 - learning_rate: 0.0030\n","Epoch 20/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 311ms/step - loss: 0.1689 - pr_auc: 0.8611 - roc_auc: 0.9821 - val_loss: 0.2422 - val_pr_auc: 0.7568 - val_roc_auc: 0.9360 - learning_rate: 0.0030\n","Epoch 21/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 320ms/step - loss: 0.1469 - pr_auc: 0.8763 - roc_auc: 0.9860 - val_loss: 0.2016 - val_pr_auc: 0.7789 - val_roc_auc: 0.9399 - learning_rate: 0.0030\n","Epoch 22/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 283ms/step - loss: 0.1356 - pr_auc: 0.8958 - roc_auc: 0.9878 - val_loss: 0.2297 - val_pr_auc: 0.7603 - val_roc_auc: 0.9395 - learning_rate: 0.0030\n","Epoch 23/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 0.1190 - pr_auc: 0.9066 - roc_auc: 0.9904\n","Epoch 23: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 285ms/step - loss: 0.1190 - pr_auc: 0.9065 - roc_auc: 0.9904 - val_loss: 0.2496 - val_pr_auc: 0.7685 - val_roc_auc: 0.9392 - learning_rate: 0.0030\n","Epoch 24/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 285ms/step - loss: 0.1135 - pr_auc: 0.9211 - roc_auc: 0.9913 - val_loss: 0.2151 - val_pr_auc: 0.7880 - val_roc_auc: 0.9358 - learning_rate: 0.0015\n","Epoch 25/25\n","\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 325ms/step - loss: 0.0984 - pr_auc: 0.9326 - roc_auc: 0.9932 - val_loss: 0.2341 - val_pr_auc: 0.7703 - val_roc_auc: 0.9347 - learning_rate: 0.0015\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 266ms/step\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step\n","{\n","  \"policy\": \"r_at_precision_0.90\",\n","  \"threshold\": 0.692307710647583,\n","  \"val_pr_auc\": 0.7931901808057307,\n","  \"val_roc_auc\": 0.9460644640248272,\n","  \"test_confusion\": {\n","    \"tn\": 3615,\n","    \"fp\": 19,\n","    \"fn\": 153,\n","    \"tp\": 211\n","  },\n","  \"test_SA_f1\": 0.7104377104377104,\n","  \"test_macro_f1\": 0.8436003737273664\n","}\n","Saved char-BiLSTM artifacts to: /content/artifacts_char_bilstm\n"]}]},{"cell_type":"markdown","source":["# Step 8: Boosting primary performance"],"metadata":{"id":"-QNj4ak8ZTb9"}},{"cell_type":"code","source":["# STEP 8.1: Train SentencePiece tokenizer (unigram) on train names only\n","!pip -q install sentencepiece==0.2.0 transformers==4.43.3 datasets==2.20.0 accelerate==0.32.1\n","\n","import sqlite3, pandas as pd, sentencepiece as spm, os, json\n","from pathlib import Path\n","\n","DB_PATH = \"./sa_names.db\"\n","TOK_DIR = Path(\"./artifacts_namebert_sa/tokenizer\")\n","TOK_DIR.mkdir(parents=True, exist_ok=True)\n","\n","def fetch_train_names():\n","    con = sqlite3.connect(DB_PATH)\n","    q = \"\"\"\n","    SELECT first_clean, last_clean, name_ascii\n","    FROM names_preprocessed\n","    WHERE name_ascii IN (SELECT name_ascii FROM names_splits WHERE split='train');\n","    \"\"\"\n","    df = pd.read_sql_query(q, con)\n","    con.close()\n","    return df\n","\n","df = fetch_train_names()\n","\n","# Build a simple training corpus (one name per line) with light augmentation\n","lines = []\n","for r in df.itertuples(index=False):\n","    first = (r.first_clean or \"\").strip()\n","    last  = (r.last_clean or \"\").strip()\n","    full  = (r.name_ascii or \"\").strip()\n","    if full:\n","        lines.append(full)\n","    if first:\n","        lines.append(first)\n","    if last:\n","        lines.append(last)\n","    # drop-view augmentations\n","    if first and last:\n","        lines.append(f\"{first} <SEP> {last}\")\n","        lines.append(first)  # simulate missing last\n","        lines.append(last)   # simulate missing first\n","\n","corpus_path = TOK_DIR / \"names_corpus.txt\"\n","with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n","    for ln in lines:\n","        f.write(ln.strip() + \"\\n\")\n","\n","# Train SentencePiece (unigram) — small vocab is good for names\n","spm.SentencePieceTrainer.Train(\n","    input=str(corpus_path),\n","    model_prefix=str(TOK_DIR / \"spm_unigram\"),\n","    vocab_size=800,\n","    model_type=\"unigram\",\n","    character_coverage=1.0,\n","    user_defined_symbols=[\"<SEP>\",\"<FIRST>\",\"<LAST>\"]\n",")\n","\n","print(\"Tokenizer files:\", list(TOK_DIR.iterdir()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23baZe-IZaSP","executionInfo":{"status":"ok","timestamp":1758480994117,"user_tz":240,"elapsed":35338,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"07c1ee7d-12d3-4b08-8e39-59b286b084ad"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mTokenizer files: [PosixPath('artifacts_namebert_sa/tokenizer/names_corpus.txt'), PosixPath('artifacts_namebert_sa/tokenizer/spm_unigram.model'), PosixPath('artifacts_namebert_sa/tokenizer/spm_unigram.vocab')]\n"]}]},{"cell_type":"code","source":["# Step: 8.2a Train a fast SentencePiece-Unigram tokenizer directly to tokenizer.json (no .model needed)\n","\n","!pip -q install tokenizers==0.19.1\n","\n","from pathlib import Path\n","from tokenizers import SentencePieceUnigramTokenizer\n","\n","TOK_DIR = Path(\"./artifacts_namebert_sa/tokenizer\")\n","TOK_DIR.mkdir(parents=True, exist_ok=True)\n","\n","corpus_path = TOK_DIR / \"names_corpus.txt\"\n","assert corpus_path.exists(), f\"Corpus not found at {corpus_path} (run Step 8.1 first).\"\n","\n","# Train directly to a fast tokenizer object\n","spu = SentencePieceUnigramTokenizer()\n","spu.train(\n","    files=[str(corpus_path)],\n","    vocab_size=800,\n","    unk_token=\"<unk>\",\n","    special_tokens=[\"<s>\", \"</s>\", \"<pad>\", \"<unk>\", \"<SEP>\", \"<FIRST>\", \"<LAST>\"]\n",")\n","\n","# Save in fast-tokenizers JSON format\n","tok_json_path = TOK_DIR / \"tokenizer.json\"\n","spu.save(str(tok_json_path))\n","\n","print(\"Fast tokenizer saved:\", tok_json_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Hzcx3b4aF7J","executionInfo":{"status":"ok","timestamp":1758481026138,"user_tz":240,"elapsed":32018,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"903a2345-0ff3-45e9-9b3c-8f393c298e67"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Fast tokenizer saved: artifacts_namebert_sa/tokenizer/tokenizer.json\n"]}]},{"cell_type":"code","source":["# STEP 8.2: Dataset + tiny RoBERTa config\n","import sqlite3, pandas as pd, numpy as np, torch, os, json\n","from datasets import Dataset, DatasetDict\n","from transformers import PreTrainedTokenizerFast, RobertaConfig, RobertaForMaskedLM, RobertaForSequenceClassification\n","from pathlib import Path\n","\n","ART_DIR = Path(\"./artifacts_namebert_sa\")\n","ART_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# Load sentencepiece as HF tokenizer\n","TOK_DIR = Path(\"./artifacts_namebert_sa/tokenizer\")\n","tok = PreTrainedTokenizerFast(\n","    tokenizer_file=str(TOK_DIR / \"tokenizer.json\"),\n","    bos_token=\"<s>\",\n","    eos_token=\"</s>\",\n","    unk_token=\"<unk>\",\n","    pad_token=\"<pad>\",\n",")\n","tok.add_special_tokens({\"additional_special_tokens\": [\"<SEP>\", \"<FIRST>\", \"<LAST>\"]})\n","\n","def fetch(split):\n","    con = sqlite3.connect(\"./sa_names.db\")\n","    q = \"\"\"\n","    SELECT first_clean, last_clean, is_south_asian\n","    FROM names_preprocessed\n","    WHERE name_ascii IN (SELECT name_ascii FROM names_splits WHERE split=?);\n","    \"\"\"\n","    df = pd.read_sql_query(q, con, params=(split,))\n","    con.close()\n","    return df\n","\n","def to_hf(split):\n","    df = fetch(split)\n","    def make_text(r):\n","        first = (r[\"first_clean\"] or \"\").strip()\n","        last  = (r[\"last_clean\"] or \"\").strip()\n","        return f\"<FIRST> {first} <SEP> <LAST> {last}\".strip()\n","    texts = df.apply(make_text, axis=1).tolist()\n","    labels = df[\"is_south_asian\"].astype(int).tolist()\n","    return Dataset.from_dict({\"text\": texts, \"label\": labels})\n","\n","ds = DatasetDict({\n","    \"train\": to_hf(\"train\"),\n","    \"validation\": to_hf(\"val\"),\n","    \"test\": to_hf(\"test\"),\n","})\n","\n","# Tokenize function\n","MAX_LEN = 48\n","def tok_fn(batch):\n","    enc = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n","    enc[\"labels\"] = batch[\"label\"]\n","    return enc\n","\n","ds_tok = ds.map(tok_fn, batched=True, remove_columns=[\"text\",\"label\"])\n","\n","# Tiny RoBERTa configs\n","config_mlm = RobertaConfig(\n","    vocab_size=len(tok),\n","    max_position_embeddings=MAX_LEN+2,\n","    hidden_size=384,\n","    num_hidden_layers=6,\n","    num_attention_heads=6,\n","    intermediate_size=768,\n","    hidden_act=\"gelu\",\n","    attention_probs_dropout_prob=0.1,\n","    hidden_dropout_prob=0.1,\n","    type_vocab_size=1,\n","    pad_token_id=tok.pad_token_id,\n","    bos_token_id=tok.convert_tokens_to_ids(\"<s>\"),\n","    eos_token_id=tok.convert_tokens_to_ids(\"</s>\")\n",")\n","\n","config_cls = RobertaConfig.from_dict({**config_mlm.to_dict(), \"num_labels\": 1, \"problem_type\": \"single_label_classification\"})\n","\n","with open(ART_DIR / \"roberta_tiny_config.json\", \"w\") as f:\n","    json.dump(config_cls.to_dict(), f, indent=2)\n","\n","print(\"Tokenizer size:\", len(tok), \"| Max len:\", MAX_LEN)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169,"referenced_widgets":["40e3590190004d15bba3ba1d415a6a0d","9e708378d02043a39def31842380c0a1","71ec213e1d6442cea8fe62d9662bc93d","4a611516447d48dea80e73b47981e644","a8a90317116d412abf7548c985025a69","4b74622285c2487db19f469b68907225","caceec925dc34f8d96793d979434bb77","6885ca96bce14e7c9d59e7f3f579e681","60bd79e1c8da4ab5b9349e2db2c14000","1cb8f49f3a3e4fb4b0618a557b30c443","486e34d4707b4e20a97779b4f261c3c3","6f2b4e0f43dc4579859bf2b929614c52","b168b832174e41a98eefd561003cc765","b676f0d7e2ad4860b4678b6ff5585622","c1ea40f6031c46c1852eb4a4a5deaa54","bb8bdc69e9c040eaa88fb917a348bffa","3c41afd1f97e4f238c04bdfc026297be","1e63e95e4bc84c6fac887af97fdc5bbd","e6cce44e7fc8439eb236d79b50eb9c88","f4ee3a275a314481a46e390b1d7b3f86","9e9f4466f6644b4daf7fff98732875e1","2c3e6ec2cef8429b9dfaaaa11bd0f0ca","68b9052f5d124d3585f58f4999f85b9e","d9acd798267f4c8c8d15261caea2838e","00db02d64e684144bd346646791dd647","b68a6c5a4a0d43efb82fe414bd58b573","d09dbd657fe841eb84368d7c6c71dde8","f7efd06083234c06aa548af64ffa67a0","418268db53f544f9bf199d3a18ec7ddf","70b0e591496e4c84a9428f1e98fde5a6","ac3187eb81d145d1a715c747b1143512","d5b848ce122c4129b53319380089ac92","bed72f99283647619d24c2cd92063870"]},"id":"UfB9HITjbuHi","executionInfo":{"status":"ok","timestamp":1758481055396,"user_tz":240,"elapsed":29259,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"863e64a7-0171-4838-b49a-c2bcd839247f"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/18651 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40e3590190004d15bba3ba1d415a6a0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/3997 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f2b4e0f43dc4579859bf2b929614c52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/3998 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68b9052f5d124d3585f58f4999f85b9e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=1` with an incompatible id to label map: {0: 'LABEL_0', 1: 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizer size: 800 | Max len: 48\n"]}]},{"cell_type":"code","source":["# Step 8.3a: add <mask> token to the fast tokenizer and sync configs\n","from pathlib import Path\n","from transformers import PreTrainedTokenizerFast, RobertaConfig\n","\n","TOK_DIR = Path(\"./artifacts_namebert_sa/tokenizer\")\n","\n","# 1) Load the fast tokenizer we trained to tokenizer.json\n","tok = PreTrainedTokenizerFast(\n","    tokenizer_file=str(TOK_DIR / \"tokenizer.json\"),\n","    bos_token=\"<s>\",\n","    eos_token=\"</s>\",\n","    unk_token=\"<unk>\",\n","    pad_token=\"<pad>\",\n",")\n","# Keep our extra specials\n","tok.add_special_tokens({\"additional_special_tokens\": [\"<SEP>\", \"<FIRST>\", \"<LAST>\"]})\n","\n","# 2) Add a proper mask token if missing\n","if tok.mask_token is None:\n","    tok.add_special_tokens({\"mask_token\": \"<mask>\"})   # ✅ set mask_token explicitly\n","\n","# 3) Save updated tokenizer (vocab size may have grown by 1)\n","tok.save_pretrained(\"./artifacts_namebert_sa/tokenizer_hf\")\n","print(\"Tokenizer saved with mask token. vocab_size:\", len(tok), \"| mask_id:\", tok.mask_token_id)\n","\n","# 4) If you already built configs earlier, reload & sync their vocab/IDs\n","# (If you don't have config_mlm/config_cls in memory, you can rebuild them now like this:)\n","MAX_LEN = 48  # use the same as before\n","config_mlm = RobertaConfig(\n","    vocab_size=len(tok),\n","    max_position_embeddings=MAX_LEN+2,\n","    hidden_size=384,\n","    num_hidden_layers=6,\n","    num_attention_heads=6,\n","    intermediate_size=768,\n","    hidden_act=\"gelu\",\n","    attention_probs_dropout_prob=0.1,\n","    hidden_dropout_prob=0.1,\n","    type_vocab_size=1,\n","    pad_token_id=tok.pad_token_id,\n","    bos_token_id=tok.bos_token_id,\n","    eos_token_id=tok.eos_token_id,\n",")\n","# RobertaConfig doesn’t have a dedicated mask_token_id field, but the model will read it from the tokenizer.\n","# We'll still persist a classifier config for later:\n","from copy import deepcopy\n","config_cls = deepcopy(config_mlm)\n","config_cls.num_labels = 1\n","config_cls.problem_type = \"single_label_classification\"\n","\n","# Save configs for clarity (optional)\n","import json, os\n","ART_DIR = Path(\"./artifacts_namebert_sa\")\n","os.makedirs(ART_DIR, exist_ok=True)\n","with open(ART_DIR / \"roberta_tiny_config.json\", \"w\") as f:\n","    json.dump(config_cls.to_dict(), f, indent=2)\n","\n","print(\"Configs synced. Hidden size/layers match earlier setup.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pO5ai1cmMiHV","executionInfo":{"status":"ok","timestamp":1758481055481,"user_tz":240,"elapsed":75,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"a2d38798-e969-4042-e2e2-e64c0ba7fa54"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizer saved with mask token. vocab_size: 801 | mask_id: 800\n","Configs synced. Hidden size/layers match earlier setup.\n"]}]},{"cell_type":"code","source":["# STEP 8.3 (Trainer-free, patched): short MLM warmup for NameBERT-SA\n","import math, json, os, torch, numpy as np\n","from torch.utils.data import DataLoader\n","from transformers import RobertaForMaskedLM, DataCollatorForLanguageModeling, get_linear_schedule_with_warmup\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", DEVICE)\n","\n","# Reuse tok, config_mlm, MAX_LEN, ART_DIR, ds from previous steps\n","train_texts = [ex[\"text\"] for ex in ds[\"train\"]]\n","\n","def encode_batch(texts):\n","    # returns dict of tensors (input_ids, attention_mask)\n","    return tok(texts, truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n","\n","class TextDataset(torch.utils.data.Dataset):\n","    def __init__(self, texts): self.texts = texts\n","    def __len__(self): return len(self.texts)\n","    def __getitem__(self, i): return self.texts[i]\n","\n","raw_ds = TextDataset(train_texts)\n","collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=True, mlm_probability=0.15)\n","\n","def collate_fn(batch_texts):\n","    enc = encode_batch(batch_texts)\n","    # ✅ collator expects a list of examples; provide a list of per-row tensors\n","    examples = [enc[\"input_ids\"][i] for i in range(enc[\"input_ids\"].size(0))]\n","    return collator(examples)  # returns a dict: {'input_ids', 'labels', 'attention_mask'}\n","\n","loader = DataLoader(raw_ds, batch_size=64, shuffle=True, drop_last=True, collate_fn=collate_fn)\n","\n","# Model (vocab size must match tok)\n","from transformers import RobertaConfig, RobertaForMaskedLM\n","model_mlm = RobertaForMaskedLM(config=config_mlm).to(DEVICE)\n","\n","# Optimizer/scheduler (use torch.optim.AdamW to avoid warning)\n","optimizer = torch.optim.AdamW(model_mlm.parameters(), lr=5e-4, weight_decay=0.01)\n","\n","epochs = 2  # keep short; bump to 3–4 later if useful\n","num_update_steps = epochs * len(loader)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=max(50, int(0.1 * num_update_steps)),\n","    num_training_steps=num_update_steps\n",")\n","\n","model_mlm.train()\n","global_step, running = 0, 0.0\n","for epoch in range(epochs):\n","    for batch in loader:\n","        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n","        out = model_mlm(**batch)\n","        loss = out.loss\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model_mlm.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        global_step += 1\n","        running += loss.item()\n","        if global_step % 100 == 0:\n","            print(f\"step {global_step}/{num_update_steps} | loss {running/100:.4f}\")\n","            running = 0.0\n","\n","# Save MLM warmup\n","os.makedirs(ART_DIR / \"mlm_manual\", exist_ok=True)\n","model_mlm.save_pretrained(ART_DIR / \"mlm_manual\")\n","tok.save_pretrained(ART_DIR / \"tokenizer_hf\")\n","print(\"MLM warmup saved to:\", str(ART_DIR / \"mlm_manual\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fag0YeQDcanN","outputId":"ff715972-af3a-4ce0-b515-f6457f46ca77","executionInfo":{"status":"ok","timestamp":1758483437940,"user_tz":240,"elapsed":2382458,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"output_type":"stream","name":"stdout","text":["step 100/582 | loss 4.6400\n","step 200/582 | loss 3.7798\n","step 300/582 | loss 3.6681\n","step 400/582 | loss 3.4929\n","step 500/582 | loss 3.4574\n","MLM warmup saved to: artifacts_namebert_sa/mlm_manual\n"]}]},{"cell_type":"code","source":["# STEP 8.4 (fixed): Fine-tune classifier (2 logits) + calibration + precision-first threshold\n","import os, json, math, sqlite3, numpy as np, torch\n","from pathlib import Path\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import (\n","    average_precision_score, roc_auc_score, precision_recall_curve,\n","    classification_report, confusion_matrix\n",")\n","from sklearn.isotonic import IsotonicRegression\n","from sklearn.utils.class_weight import compute_class_weight\n","from transformers import (\n","    PreTrainedTokenizerFast, RobertaConfig, RobertaForMaskedLM,\n","    RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",")\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", DEVICE)\n","\n","ART_DIR = Path(\"./artifacts_namebert_sa\")\n","TOK_DIR = ART_DIR / \"tokenizer_hf\"          # from earlier steps\n","MLM_DIR = ART_DIR / \"mlm_manual\"            # from Step 8.3 (trainer-free)\n","CLS_DIR = ART_DIR / \"cls_manual\"\n","CLS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# ---------- (Re)load tokenizer, config, datasets if needed ----------\n","try:\n","    tok\n","    ds_tok\n","    MAX_LEN\n","except NameError:\n","    from transformers import PreTrainedTokenizerFast\n","    import pandas as pd, datasets as hfds\n","    tok = PreTrainedTokenizerFast.from_pretrained(str(TOK_DIR))\n","    MAX_LEN = 48\n","\n","    # Rebuild tokenized datasets quickly from SQLite\n","    def fetch(split):\n","        import sqlite3\n","        con = sqlite3.connect(\"./sa_names.db\")\n","        q = \"\"\"\n","        SELECT first_clean, last_clean, is_south_asian\n","        FROM names_preprocessed\n","        WHERE name_ascii IN (SELECT name_ascii FROM names_splits WHERE split=?);\n","        \"\"\"\n","        df = pd.read_sql_query(q, con, params=(split,))\n","        con.close()\n","        return df\n","\n","    def to_hf(split):\n","        df = fetch(split)\n","        texts = (\"<FIRST> \" + df[\"first_clean\"].fillna(\"\") +\n","                 \" <SEP> <LAST> \" + df[\"last_clean\"].fillna(\"\")).tolist()\n","        labels = df[\"is_south_asian\"].astype(int).tolist()\n","        return hfds.Dataset.from_dict({\"text\": texts, \"label\": labels})\n","\n","    ds = hfds.DatasetDict({\"train\": to_hf(\"train\"),\n","                           \"validation\": to_hf(\"val\"),\n","                           \"test\": to_hf(\"test\")})\n","\n","    def tok_fn(batch):\n","        enc = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n","        enc[\"labels\"] = batch[\"label\"]\n","        return enc\n","\n","    ds_tok = ds.map(tok_fn, batched=True, remove_columns=[\"text\",\"label\"])\n","\n","# Two-logit classifier config (binary via softmax)\n","config_cls = RobertaConfig(\n","    vocab_size=len(tok),\n","    max_position_embeddings=MAX_LEN+2,\n","    hidden_size=384,\n","    num_hidden_layers=6,\n","    num_attention_heads=6,\n","    intermediate_size=768,\n","    hidden_act=\"gelu\",\n","    attention_probs_dropout_prob=0.1,\n","    hidden_dropout_prob=0.1,\n","    type_vocab_size=1,\n","    pad_token_id=tok.pad_token_id,\n","    bos_token_id=tok.bos_token_id,\n","    eos_token_id=tok.eos_token_id,\n","    num_labels=2,\n","    problem_type=\"single_label_classification\",\n","    id2label={0: \"non_sa\", 1: \"sa\"},\n","    label2id={\"non_sa\": 0, \"sa\": 1},\n",")\n","\n","# ---------- Datasets -> PyTorch ----------\n","class HFDatasetWrapper(Dataset):\n","    def __init__(self, ds_split): self.ds = ds_split\n","    def __len__(self): return len(self.ds)\n","    def __getitem__(self, i):\n","        ex = self.ds[i]\n","        return {\n","            \"input_ids\": torch.tensor(ex[\"input_ids\"], dtype=torch.long),\n","            \"attention_mask\": torch.tensor(ex[\"attention_mask\"], dtype=torch.long),\n","            \"labels\": torch.tensor(ex[\"labels\"], dtype=torch.long),   # int labels (0/1) for CE\n","        }\n","\n","train_ds = HFDatasetWrapper(ds_tok[\"train\"])\n","val_ds   = HFDatasetWrapper(ds_tok[\"validation\"])\n","test_ds  = HFDatasetWrapper(ds_tok[\"test\"])\n","\n","train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n","val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False)\n","test_loader  = DataLoader(test_ds,  batch_size=128, shuffle=False)\n","\n","# ---------- Class weights for imbalance ----------\n","y_tr = np.array([ds_tok[\"train\"][i][\"labels\"] for i in range(len(ds_tok[\"train\"]))], dtype=int)\n","cw = compute_class_weight(\"balanced\", classes=np.array([0,1], int), y=y_tr)\n","w0, w1 = float(cw[0]), float(cw[1])\n","print(\"Class weights:\", {\"non_sa\": w0, \"sa\": w1})\n","ce_weight = torch.tensor([w0, w1], dtype=torch.float, device=DEVICE)\n","\n","# ---------- Model: init encoder from MLM warmup, 2-logit head ----------\n","mlm_model = RobertaForMaskedLM.from_pretrained(str(MLM_DIR))\n","model = RobertaForSequenceClassification(config=config_cls)\n","model.roberta.load_state_dict(mlm_model.roberta.state_dict(), strict=False)\n","del mlm_model\n","model.to(DEVICE)\n","\n","# ---------- Optimizer & scheduler ----------\n","optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n","EPOCHS = 6\n","num_training_steps = EPOCHS * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=max(100, int(0.1 * num_training_steps)), num_training_steps=num_training_steps\n",")\n","criterion = torch.nn.CrossEntropyLoss(weight=ce_weight)\n","\n","# ---------- Helper to evaluate probs ----------\n","def eval_probs(dloader):\n","    model.eval()\n","    probs, labels = [], []\n","    with torch.no_grad():\n","        for batch in dloader:\n","            inputs = {k: v.to(DEVICE) for k,v in batch.items() if k in (\"input_ids\",\"attention_mask\")}\n","            y = batch[\"labels\"].numpy()\n","            logits = model(**inputs).logits         # (B,2)\n","            p = torch.softmax(logits, dim=-1)[:,1]  # P(sa)\n","            probs.append(p.cpu().numpy()); labels.append(y)\n","    return np.concatenate(probs), np.concatenate(labels)\n","\n","# ---------- Train with early stopping on val PR-AUC ----------\n","best_val_prauc = -1.0\n","best_state = None\n","patience, best_epoch = 3, -1\n","no_improve = 0\n","\n","for epoch in range(1, EPOCHS+1):\n","    model.train()\n","    running = 0.0\n","    for batch in train_loader:\n","        inputs = {k: v.to(DEVICE) for k,v in batch.items() if k in (\"input_ids\",\"attention_mask\")}\n","        labels = batch[\"labels\"].to(DEVICE)         # long, shape (B,)\n","        logits = model(**inputs).logits             # (B,2)\n","        loss = criterion(logits, labels)            # CE with class weights\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step(); scheduler.step(); optimizer.zero_grad()\n","        running += loss.item()\n","\n","    p_va, y_va = eval_probs(val_loader)\n","    pra = average_precision_score(y_va, p_va)\n","    try:\n","        roa = roc_auc_score(y_va, p_va)\n","    except:\n","        roa = float(\"nan\")\n","    print(f\"Epoch {epoch}/{EPOCHS}  train_loss={running/len(train_loader):.4f}  val_PR-AUC={pra:.4f}  val_ROC-AUC={roa:.4f}\")\n","    if pra > best_val_prauc + 1e-4:\n","        best_val_prauc = pra\n","        best_state = {k: v.cpu().clone() for k,v in model.state_dict().items()}\n","        best_epoch = epoch\n","        no_improve = 0\n","    else:\n","        no_improve += 1\n","        if no_improve >= patience:\n","            print(f\"Early stopping at epoch {epoch} (best epoch {best_epoch}, PR-AUC={best_val_prauc:.4f})\")\n","            break\n","\n","# Restore best\n","if best_state is not None:\n","    model.load_state_dict(best_state)\n","\n","# ---------- Calibrate (isotonic) on VAL, choose precision-first threshold ----------\n","iso = IsotonicRegression(out_of_bounds=\"clip\")\n","iso.fit(p_va, y_va)                         # use last computed p_va/y_va\n","cp_va = np.clip(iso.predict(p_va), 0, 1)\n","\n","prec, rec, thr = precision_recall_curve(y_va, cp_va)\n","thr = np.concatenate([thr, [1.0]])\n","mask = np.where(prec >= 0.90)[0]\n","if len(mask)==0:\n","    f1 = np.where((prec+rec)>0, 2*prec*rec/(prec+rec), 0.0)\n","    i = int(np.argmax(f1)); thr_star = float(thr[i]); policy=\"best_f1_fallback\"\n","else:\n","    i = mask[np.argmax(rec[mask])]; thr_star = float(thr[i]); policy=\"r_at_precision_0.90\"\n","\n","# ---------- Test eval ----------\n","p_te, y_te = eval_probs(test_loader)\n","cp_te = np.clip(iso.predict(p_te), 0, 1)\n","y_hat = (cp_te >= thr_star).astype(int)\n","\n","cm = confusion_matrix(y_te, y_hat, labels=[0,1])\n","rpt = classification_report(y_te, y_hat, labels=[0,1], target_names=[\"non-SA\",\"SA\"], output_dict=True)\n","report = {\n","    \"policy\": policy,\n","    \"threshold\": thr_star,\n","    \"val_pr_auc\": float(average_precision_score(y_va, cp_va)),\n","    \"val_roc_auc\": float(roc_auc_score(y_va, cp_va)),\n","    \"test_confusion\": {\"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]), \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1])},\n","    \"test_SA_f1\": float(rpt[\"SA\"][\"f1-score\"]),\n","    \"test_macro_f1\": float(rpt[\"macro avg\"][\"f1-score\"]),\n","}\n","print(json.dumps(report, indent=2))\n","\n","# ---------- Save artifacts ----------\n","torch.save(model.state_dict(), CLS_DIR / \"roberta_cls_state.pt\")\n","import joblib\n","joblib.dump(iso, ART_DIR / \"cal_isotonic_cls.joblib\")\n","with open(ART_DIR / \"inference_config.json\",\"w\") as f:\n","    json.dump({\"threshold\": thr_star, \"policy\": policy,\n","               \"abstain_band\": [max(0.0, thr_star-0.10), min(1.0, thr_star+0.10)]}, f, indent=2)\n","print(\"Saved NameBERT-SA classifier + calibrator to:\", str(ART_DIR.resolve()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zHKxXMCaENwP","outputId":"49b134f7-fcd2-4310-9de1-629c915a56ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cpu\n","Class weights: {'non_sa': 0.5500147449130051, 'sa': 5.498525943396227}\n"]}]},{"cell_type":"markdown","source":["# Step 9: Creating a Combination Model"],"metadata":{"id":"-2Pmh9Gslhem"}},{"cell_type":"code","source":["# STEP 9: Calibrated ensemble of tuned SGD (fusion) + NameBERT-SA classifier\n","import os, json, sqlite3, numpy as np, pandas as pd, torch, joblib\n","from pathlib import Path\n","from sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix, average_precision_score, roc_auc_score\n","from sklearn.isotonic import IsotonicRegression\n","\n","# -------- Paths --------\n","DB_PATH   = \"./sa_names.db\"\n","SGD_DIR   = Path(\"./artifacts_tuned_sgd\")       # from Step 6 (fast SGD)\n","NB_DIR    = Path(\"./artifacts_namebert_sa\")     # from Step 8.4 (trainer-free)\n","ENS_DIR   = Path(\"./artifacts_ensemble\")\n","ENS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# -------- Load SGD artifacts --------\n","vec_full  = joblib.load(SGD_DIR / \"vec_full_tfidf.joblib\")\n","vec_first = joblib.load(SGD_DIR / \"vec_first_tfidf.joblib\")\n","vec_last  = joblib.load(SGD_DIR / \"vec_last_tfidf.joblib\")\n","sgd_full  = joblib.load(SGD_DIR / \"sgd_full_tuned.joblib\")\n","sgd_first = joblib.load(SGD_DIR / \"sgd_first_tuned.joblib\")\n","sgd_last  = joblib.load(SGD_DIR / \"sgd_last_tuned.joblib\")\n","cal_full  = joblib.load(SGD_DIR / \"cal_full_isotonic.joblib\")\n","cal_first = joblib.load(SGD_DIR / \"cal_first_isotonic.joblib\")\n","cal_last  = joblib.load(SGD_DIR / \"cal_last_isotonic.joblib\")\n","\n","# -------- Load NameBERT-SA artifacts --------\n","from transformers import PreTrainedTokenizerFast, RobertaConfig, RobertaForSequenceClassification\n","tok = PreTrainedTokenizerFast.from_pretrained(str(NB_DIR / \"tokenizer_hf\"))\n","MAX_LEN = 48\n","config_cls = RobertaConfig(\n","    vocab_size=len(tok), max_position_embeddings=MAX_LEN+2,\n","    hidden_size=384, num_hidden_layers=6, num_attention_heads=6, intermediate_size=768,\n","    hidden_act=\"gelu\", attention_probs_dropout_prob=0.1, hidden_dropout_prob=0.1,\n","    type_vocab_size=1, pad_token_id=tok.pad_token_id, bos_token_id=tok.bos_token_id, eos_token_id=tok.eos_token_id,\n","    num_labels=2, problem_type=\"single_label_classification\", id2label={0:\"non_sa\",1:\"sa\"}, label2id={\"non_sa\":0,\"sa\":1}\n",")\n","nb_model = RobertaForSequenceClassification(config=config_cls)\n","state = torch.load(NB_DIR / \"cls_manual/roberta_cls_state.pt\", map_location=\"cpu\")\n","nb_model.load_state_dict(state, strict=True)\n","nb_model.eval()\n","iso_nb = joblib.load(NB_DIR / \"cal_isotonic_cls.joblib\")\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","nb_model.to(DEVICE)\n","\n","# -------- Data helpers --------\n","def fetch_split(split: str):\n","    con = sqlite3.connect(DB_PATH)\n","    q = \"\"\"\n","    SELECT name_ascii, first_clean, last_clean, is_south_asian\n","    FROM names_preprocessed\n","    WHERE name_ascii IN (SELECT name_ascii FROM names_splits WHERE split=?);\n","    \"\"\"\n","    df = pd.read_sql_query(q, con, params=(split,))\n","    con.close()\n","    return df\n","\n","val_df  = fetch_split(\"val\")\n","test_df = fetch_split(\"test\")\n","y_va = val_df[\"is_south_asian\"].to_numpy(int)\n","y_te = test_df[\"is_south_asian\"].to_numpy(int)\n","\n","# -------- SGD calibrated fusion probs --------\n","def sgd_calibrated_fusion(df: pd.DataFrame) -> np.ndarray:\n","    Xf  = vec_full.transform(df[\"name_ascii\"].fillna(\"\"))\n","    Xfi = vec_first.transform(df[\"first_clean\"].fillna(\"\"))\n","    Xla = vec_last.transform(df[\"last_clean\"].fillna(\"\"))\n","\n","    p_first = sgd_first.predict_proba(Xfi)[:,1]\n","    p_last  = sgd_last.predict_proba(Xla)[:,1]\n","\n","    # Calibrate per-view\n","    cp_first = np.clip(cal_first.predict(p_first), 0, 1)\n","    cp_last  = np.clip(cal_last.predict(p_last),  0, 1)\n","\n","    # Fusion: OR in probability space\n","    cp_fusion = 1.0 - (1.0 - cp_first) * (1.0 - cp_last)\n","    return cp_fusion\n","\n","cp_sgd_va = sgd_calibrated_fusion(val_df)\n","cp_sgd_te = sgd_calibrated_fusion(test_df)\n","\n","# -------- NameBERT-SA calibrated probs --------\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","\n","class SimpleDS(Dataset):\n","    def __init__(self, first, last, labels=None):\n","        self.first, self.last, self.labels = list(first), list(last), labels\n","    def __len__(self): return len(self.first)\n","    def __getitem__(self, i):\n","        text = f\"<FIRST> {self.first[i] or ''} <SEP> <LAST> {self.last[i] or ''}\"\n","        enc = tok(text, truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n","        ex = {k: enc[k][0] for k in (\"input_ids\",\"attention_mask\")}\n","        if self.labels is not None:\n","            ex[\"labels\"] = torch.tensor(int(self.labels[i]), dtype=torch.long)\n","        return ex\n","\n","def nb_probs(df: pd.DataFrame) -> np.ndarray:\n","    ds = SimpleDS(df[\"first_clean\"], df[\"last_clean\"], None)\n","    dl = DataLoader(ds, batch_size=128, shuffle=False)\n","    probs = []\n","    with torch.no_grad():\n","        for batch in dl:\n","            inputs = {k: v.to(DEVICE) for k,v in batch.items() if k in (\"input_ids\",\"attention_mask\")}\n","            logits = nb_model(**inputs).logits              # (B,2)\n","            p = torch.softmax(logits, dim=-1)[:,1].cpu().numpy()\n","            probs.append(p)\n","    p = np.concatenate(probs)\n","    # Calibrate with isotonic fitted on val in Step 8.4\n","    cp = np.clip(iso_nb.predict(p), 0, 1)\n","    return cp\n","\n","cp_nb_va = nb_probs(val_df)\n","cp_nb_te = nb_probs(test_df)\n","\n","# -------- Ensemble & threshold selection --------\n","w_sgd, w_nb = 1.0, 1.0   # start with equal weights; you can tune later\n","cp_ens_va = (w_sgd*cp_sgd_va + w_nb*cp_nb_va) / (w_sgd + w_nb)\n","cp_ens_te = (w_sgd*cp_sgd_te + w_nb*cp_nb_te) / (w_sgd + w_nb)\n","\n","# Precision-first threshold on validation (R@P >= 0.90), fallback best-F1\n","from sklearn.metrics import precision_recall_curve\n","prec, rec, thr = precision_recall_curve(y_va, cp_ens_va)\n","thr = np.concatenate([thr, [1.0]])\n","mask = np.where(prec >= 0.90)[0]\n","if len(mask)==0:\n","    f1 = np.where((prec+rec)>0, 2*prec*rec/(prec+rec), 0.0)\n","    i = int(np.argmax(f1)); thr_star = float(thr[i]); policy=\"best_f1_fallback\"\n","else:\n","    i = mask[np.argmax(rec[mask])]; thr_star = float(thr[i]); policy=\"r_at_precision_0.90\"\n","\n","# -------- Test evaluation --------\n","from sklearn.metrics import classification_report, confusion_matrix\n","y_hat = (cp_ens_te >= thr_star).astype(int)\n","cm = confusion_matrix(y_te, y_hat, labels=[0,1])\n","rpt = classification_report(y_te, y_hat, labels=[0,1], target_names=[\"non-SA\",\"SA\"], output_dict=True)\n","report = {\n","    \"policy\": policy,\n","    \"threshold\": thr_star,\n","    \"val_pr_auc\": float(average_precision_score(y_va, cp_ens_va)),\n","    \"val_roc_auc\": float(roc_auc_score(y_va, cp_ens_va)),\n","    \"test_confusion\": {\"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]), \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1])},\n","    \"test_SA_f1\": float(rpt[\"SA\"][\"f1-score\"]),\n","    \"test_macro_f1\": float(rpt[\"macro avg\"][\"f1-score\"]),\n","}\n","print(json.dumps(report, indent=2))\n","\n","# -------- Save inference helper (ensemble) --------\n","INF_PATH = ENS_DIR / \"inference_ensemble.py\"\n","with open(INF_PATH, \"w\") as f:\n","    f.write(r'''\n","import json, torch, numpy as np, joblib\n","from pathlib import Path\n","from transformers import PreTrainedTokenizerFast, RobertaConfig, RobertaForSequenceClassification\n","\n","class EnsemblePredictor:\n","    def __init__(self, db_path=None):\n","        # Paths\n","        self.SGD_DIR = Path(\"./artifacts_tuned_sgd\")\n","        self.NB_DIR  = Path(\"./artifacts_namebert_sa\")\n","        self.ENS_DIR = Path(\"./artifacts_ensemble\")\n","\n","        # Load SGD vecs/models/calibrators\n","        self.vec_first = joblib.load(self.SGD_DIR / \"vec_first_tfidf.joblib\")\n","        self.vec_last  = joblib.load(self.SGD_DIR / \"vec_last_tfidf.joblib\")\n","        self.sgd_first = joblib.load(self.SGD_DIR / \"sgd_first_tuned.joblib\")\n","        self.sgd_last  = joblib.load(self.SGD_DIR / \"sgd_last_tuned.joblib\")\n","        self.cal_first = joblib.load(self.SGD_DIR / \"cal_first_isotonic.joblib\")\n","        self.cal_last  = joblib.load(self.SGD_DIR / \"cal_last_isotonic.joblib\")\n","\n","        # Load NameBERT-SA\n","        self.tok = PreTrainedTokenizerFast.from_pretrained(str(self.NB_DIR / \"tokenizer_hf\"))\n","        cfg = RobertaConfig(\n","            vocab_size=len(self.tok), max_position_embeddings=50,\n","            hidden_size=384, num_hidden_layers=6, num_attention_heads=6, intermediate_size=768,\n","            hidden_act=\"gelu\", attention_probs_dropout_prob=0.1, hidden_dropout_prob=0.1,\n","            type_vocab_size=1, pad_token_id=self.tok.pad_token_id, bos_token_id=self.tok.bos_token_id, eos_token_id=self.tok.eos_token_id,\n","            num_labels=2, problem_type=\"single_label_classification\", id2label={0:\"non_sa\",1:\"sa\"}, label2id={\"non_sa\":0,\"sa\":1}\n","        )\n","        self.nb_model = RobertaForSequenceClassification(cfg)\n","        state = torch.load(self.NB_DIR / \"cls_manual/roberta_cls_state.pt\", map_location=\"cpu\")\n","        self.nb_model.load_state_dict(state, strict=True)\n","        self.nb_model.eval()\n","        self.iso_nb = joblib.load(self.NB_DIR / \"cal_isotonic_cls.joblib\")\n","\n","        # Ensemble config\n","        self.cfg = json.load(open(self.ENS_DIR / \"inference_config.json\"))\n","        self.THR = self.cfg[\"threshold\"]\n","        self.ABSTAIN_L, self.ABSTAIN_H = self.cfg[\"abstain_band\"]\n","        self.w_sgd = self.cfg.get(\"w_sgd\", 1.0)\n","        self.w_nb  = self.cfg.get(\"w_nb\", 1.0)\n","\n","    @staticmethod\n","    def _split(name: str):\n","        name = (name or \"\").strip().lower()\n","        parts = name.split()\n","        if len(parts)==0: return \"\", \"\"\n","        if len(parts)==1: return parts[0], \"\"\n","        return parts[0], parts[-1]\n","\n","    def _sgd_prob(self, name: str):\n","        first, last = self._split(name)\n","        Xfi = self.vec_first.transform([first])\n","        Xla = self.vec_last.transform([last])\n","        p_first = self.sgd_first.predict_proba(Xfi)[:,1]\n","        p_last  = self.sgd_last.predict_proba(Xla)[:,1]\n","        cp_first = np.clip(self.cal_first.predict(p_first), 0, 1)[0]\n","        cp_last  = np.clip(self.cal_last.predict(p_last),  0, 1)[0]\n","        return 1.0 - (1.0 - cp_first) * (1.0 - cp_last)\n","\n","    def _nb_prob(self, name: str):\n","        first, last = self._split(name)\n","        text = f\"<FIRST> {first} <SEP> <LAST> {last}\"\n","        enc = self.tok(text, truncation=True, padding=\"max_length\", max_length=48, return_tensors=\"pt\")\n","        with torch.no_grad():\n","            logits = self.nb_model(input_ids=enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"]).logits\n","            p = torch.softmax(logits, dim=-1)[:,1].item()\n","        return float(np.clip(self.iso_nb.predict([p])[0], 0, 1))\n","\n","    def predict(self, name: str):\n","        ps = self._sgd_prob(name)\n","        pn = self._nb_prob(name)\n","        p  = (self.w_sgd*ps + self.w_nb*pn) / (self.w_sgd + self.w_nb)\n","\n","        if p < self.ABSTAIN_L: decision = \"non_sa\"\n","        elif p > self.ABSTAIN_H: decision = \"sa\"\n","        else: decision = \"abstain\"\n","        hard = \"sa\" if p >= self.THR else \"non_sa\"\n","\n","        return {\"name\": name, \"prob_sgd\": ps, \"prob_nb\": pn, \"prob_ens\": p,\n","                \"decision_abstain_band\": decision,\n","                \"hard_decision_at_threshold\": {\"threshold\": self.THR, \"label\": hard}}\n","''')\n","\n","# Save config for inference\n","cfg = {\n","    \"threshold\": thr_star,\n","    \"policy\": policy,\n","    \"abstain_band\": [max(0.0, thr_star-0.10), min(1.0, thr_star+0.10)],\n","    \"w_sgd\": float(w_sgd),\n","    \"w_nb\":  float(w_nb),\n","}\n","json.dump(cfg, open(ENS_DIR / \"inference_config.json\",\"w\"), indent=2)\n","\n","print(\"Saved ensemble artifacts to:\", str(ENS_DIR.resolve()))"],"metadata":{"id":"10xQOXwflreg","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"error","timestamp":1758484742762,"user_tz":240,"elapsed":524,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}},"outputId":"88f6163b-0693-4595-a555-cd98f20b09af"},"execution_count":18,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'artifacts_namebert_sa/cls_manual/roberta_cls_state.pt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4086433591.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[0mnb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaForSequenceClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNB_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"cls_manual/roberta_cls_state.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'artifacts_namebert_sa/cls_manual/roberta_cls_state.pt'"]}]},{"cell_type":"markdown","source":["# CSV Scorer Script"],"metadata":{"id":"P3JhkHX643qu"}},{"cell_type":"code","source":["# batch_score.py\n","# Score a CSV of names and write probabilities + decisions.\n","# Usage:\n","#   python batch_score.py --in input.csv --out scored.csv --name-col name\n","# Optional:\n","#   --threshold 0.59 --abstain-low 0.49 --abstain-high 0.69\n","\n","import argparse\n","import csv\n","import json\n","from pathlib import Path\n","import numpy as np\n","import torch\n","import joblib\n","from transformers import PreTrainedTokenizerFast, RobertaConfig, RobertaForSequenceClassification\n","\n","# Paths (adjust if needed)\n","ROOT      = Path(\".\")\n","SGD_DIR   = ROOT / \"artifacts_tuned_sgd\"\n","NB_DIR    = ROOT / \"artifacts_namebert_sa\"\n","ENS_DIR   = ROOT / \"artifacts_ensemble\"\n","\n","class EnsemblePredictor:\n","    def __init__(self):\n","        self.vec_first = joblib.load(SGD_DIR / \"vec_first_tfidf.joblib\")\n","        self.vec_last  = joblib.load(SGD_DIR / \"vec_last_tfidf.joblib\")\n","        self.sgd_first = joblib.load(SGD_DIR / \"sgd_first_tuned.joblib\")\n","        self.sgd_last  = joblib.load(SGD_DIR / \"sgd_last_tuned.joblib\")\n","        self.cal_first = joblib.load(SGD_DIR / \"cal_first_isotonic.joblib\")\n","        self.cal_last  = joblib.load(SGD_DIR / \"cal_last_isotonic.joblib\")\n","\n","        self.tok = PreTrainedTokenizerFast.from_pretrained(str(NB_DIR / \"tokenizer_hf\"))\n","        self.MAX_LEN = 48\n","        cfg = RobertaConfig(\n","            vocab_size=len(self.tok), max_position_embeddings=self.MAX_LEN + 2,\n","            hidden_size=384, num_hidden_layers=6, num_attention_heads=6, intermediate_size=768,\n","            hidden_act=\"gelu\", attention_probs_dropout_prob=0.1, hidden_dropout_prob=0.1,\n","            type_vocab_size=1, pad_token_id=self.tok.pad_token_id, bos_token_id=self.tok.bos_token_id,\n","            eos_token_id=self.tok.eos_token_id, num_labels=2, problem_type=\"single_label_classification\",\n","            id2label={0: \"non_sa\", 1: \"sa\"}, label2id={\"non_sa\": 0, \"sa\": 1},\n","        )\n","        self.nb_model = RobertaForSequenceClassification(cfg)\n","        state = torch.load(NB_DIR / \"cls_manual\" / \"roberta_cls_state.pt\", map_location=\"cpu\")\n","        self.nb_model.load_state_dict(state, strict=True)\n","        self.nb_model.eval()\n","        self.iso_nb = joblib.load(NB_DIR / \"cal_isotonic_cls.joblib\")\n","\n","        cfg_js = json.load(open(ENS_DIR / \"inference_config.json\"))\n","        self.default_thr = float(cfg_js[\"threshold\"])\n","        self.abstain_l, self.abstain_h = [float(x) for x in cfg_js[\"abstain_band\"]]\n","        self.w_sgd = float(cfg_js.get(\"w_sgd\", 1.0))\n","        self.w_nb  = float(cfg_js.get(\"w_nb\", 1.0))\n","\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.nb_model.to(self.device)\n","\n","    @staticmethod\n","    def _split(name: str):\n","        s = (name or \"\").strip().lower()\n","        parts = s.split()\n","        if len(parts) == 0: return \"\", \"\"\n","        if len(parts) == 1: return parts[0], \"\"\n","        return parts[0], parts[-1]\n","\n","    def _sgd_prob(self, name: str) -> float:\n","        first, last = self._split(name)\n","        Xfi = self.vec_first.transform([first])\n","        Xla = self.vec_last.transform([last])\n","        p_first = self.sgd_first.predict_proba(Xfi)[:, 1]\n","        p_last  = self.sgd_last.predict_proba(Xla)[:, 1]\n","        cp_first = np.clip(self.cal_first.predict(p_first), 0, 1)[0]\n","        cp_last  = np.clip(self.cal_last.predict(p_last),  0, 1)[0]\n","        return float(1.0 - (1.0 - cp_first) * (1.0 - cp_last))\n","\n","    def _nb_prob(self, name: str) -> float:\n","        first, last = self._split(name)\n","        text = f\"<FIRST> {first} <SEP> <LAST> {last}\"\n","        enc = self.tok(text, truncation=True, padding=\"max_length\",\n","                       max_length=self.MAX_LEN, return_tensors=\"pt\")\n","        with torch.no_grad():\n","            logits = self.nb_model(\n","                input_ids=enc[\"input_ids\"].to(self.device),\n","                attention_mask=enc[\"attention_mask\"].to(self.device)\n","            ).logits\n","            p = torch.softmax(logits, dim=-1)[:, 1].item()\n","        return float(np.clip(self.iso_nb.predict([p])[0], 0, 1))\n","\n","    def predict(self, name: str, threshold: float, abstain_l: float, abstain_h: float):\n","        ps = self._sgd_prob(name)\n","        pn = self._nb_prob(name)\n","        p  = (self.w_sgd * ps + self.w_nb * pn) / (self.w_sgd + self.w_nb)\n","\n","        if p < abstain_l: decision = \"non_sa\"\n","        elif p > abstain_h: decision = \"sa\"\n","        else: decision = \"abstain\"\n","        hard = \"sa\" if p >= threshold else \"non_sa\"\n","\n","        return p, ps, pn, decision, hard\n","\n","def main():\n","    ap = argparse.ArgumentParser()\n","    ap.add_argument(\"--in\", dest=\"inp\", required=True, help=\"Input CSV path\")\n","    ap.add_argument(\"--out\", dest=\"out\", required=True, help=\"Output CSV path\")\n","    ap.add_argument(\"--name-col\", dest=\"name_col\", default=\"name\", help=\"Column with full name\")\n","    ap.add_argument(\"--threshold\", type=float, default=None, help=\"Override decision threshold (0..1)\")\n","    ap.add_argument(\"--abstain-low\", type=float, default=None, help=\"Override abstain low\")\n","    ap.add_argument(\"--abstain-high\", type=float, default=None, help=\"Override abstain high\")\n","    args = ap.parse_args()\n","\n","    pred = EnsemblePredictor()\n","    thr = pred.default_thr if args.threshold is None else float(args.threshold)\n","    a_low = pred.abstain_l if args.abstain_low is None else float(args.abstain_low)\n","    a_high = pred.abstain_h if args.abstain_high is None else float(args.abstain_high)\n","\n","    rows = []\n","    with open(args.inp, newline=\"\", encoding=\"utf-8\") as f:\n","        reader = csv.DictReader(f)\n","        if args.name_col not in reader.fieldnames:\n","            raise ValueError(f\"Column '{args.name_col}' not found. Available: {reader.fieldnames}\")\n","        for r in reader:\n","            name = r[args.name_col]\n","            p, ps, pn, decision, hard = pred.predict(name, thr, a_low, a_high)\n","            r_out = dict(r)\n","            r_out.update({\n","                \"prob_ensemble\": f\"{p:.6f}\",\n","                \"prob_sgd\": f\"{ps:.6f}\",\n","                \"prob_namebert\": f\"{pn:.6f}\",\n","                \"decision_abstain_band\": decision,\n","                \"hard_label\": hard,\n","                \"threshold_used\": thr,\n","                \"abstain_low\": a_low,\n","                \"abstain_high\": a_high,\n","            })\n","            rows.append(r_out)\n","\n","    out_fields = list(rows[0].keys()) if rows else None\n","    with open(args.out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n","        writer = csv.DictWriter(f, fieldnames=out_fields)\n","        writer.writeheader()\n","        writer.writerows(rows)\n","    print(f\"Wrote {len(rows)} rows to {args.out}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"9s_qIdnZ5BC7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How to run it can be found below, as an example."],"metadata":{"id":"vKA-SF2m5Etj"}},{"cell_type":"code","source":["pip install torch transformers joblib numpy\n","\n","# Example: score a CSV with a \"name\" column\n","python batch_score.py --in providers.csv --out providers_scored.csv --name-col name\n","\n","# Optional policy overrides\n","python batch_score.py --in providers.csv --out providers_scored.csv \\\n","  --name-col full_name --threshold 0.60 --abstain-low 0.50 --abstain-high 0.70"],"metadata":{"id":"ctcyZ06U5HKE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creating an API Service for the Model"],"metadata":{"id":"UyLVwndFpBRA"}},{"cell_type":"code","source":["# inference_service.py\n","# FastAPI service for South Asian name classification (calibrated ensemble).\n","# Endpoints:\n","#   GET  /health\n","#   POST /predict        -> single name\n","#   POST /predict_batch  -> list of names\n","\n","import os\n","import json\n","from pathlib import Path\n","from typing import List, Optional\n","\n","import numpy as np\n","import torch\n","import joblib\n","from fastapi import FastAPI\n","from pydantic import BaseModel, Field\n","from transformers import PreTrainedTokenizerFast, RobertaConfig, RobertaForSequenceClassification\n","\n","# ---------- Paths (adjust if needed) ----------\n","ROOT      = Path(\".\")\n","SGD_DIR   = ROOT / \"artifacts_tuned_sgd\"\n","NB_DIR    = ROOT / \"artifacts_namebert_sa\"\n","ENS_DIR   = ROOT / \"artifacts_ensemble\"\n","\n","# ---------- Ensemble Predictor ----------\n","class EnsemblePredictor:\n","    def __init__(self):\n","        # Load SGD artifacts\n","        self.vec_first = joblib.load(SGD_DIR / \"vec_first_tfidf.joblib\")\n","        self.vec_last  = joblib.load(SGD_DIR / \"vec_last_tfidf.joblib\")\n","        self.sgd_first = joblib.load(SGD_DIR / \"sgd_first_tuned.joblib\")\n","        self.sgd_last  = joblib.load(SGD_DIR / \"sgd_last_tuned.joblib\")\n","        self.cal_first = joblib.load(SGD_DIR / \"cal_first_isotonic.joblib\")\n","        self.cal_last  = joblib.load(SGD_DIR / \"cal_last_isotonic.joblib\")\n","\n","        # Load NameBERT-SA\n","        self.tok = PreTrainedTokenizerFast.from_pretrained(str(NB_DIR / \"tokenizer_hf\"))\n","        self.MAX_LEN = 48\n","        cfg = RobertaConfig(\n","            vocab_size=len(self.tok),\n","            max_position_embeddings=self.MAX_LEN + 2,\n","            hidden_size=384,\n","            num_hidden_layers=6,\n","            num_attention_heads=6,\n","            intermediate_size=768,\n","            hidden_act=\"gelu\",\n","            attention_probs_dropout_prob=0.1,\n","            hidden_dropout_prob=0.1,\n","            type_vocab_size=1,\n","            pad_token_id=self.tok.pad_token_id,\n","            bos_token_id=self.tok.bos_token_id,\n","            eos_token_id=self.tok.eos_token_id,\n","            num_labels=2,\n","            problem_type=\"single_label_classification\",\n","            id2label={0: \"non_sa\", 1: \"sa\"},\n","            label2id={\"non_sa\": 0, \"sa\": 1},\n","        )\n","        self.nb_model = RobertaForSequenceClassification(cfg)\n","        state = torch.load(NB_DIR / \"cls_manual\" / \"roberta_cls_state.pt\", map_location=\"cpu\")\n","        self.nb_model.load_state_dict(state, strict=True)\n","        self.nb_model.eval()\n","        self.iso_nb = joblib.load(NB_DIR / \"cal_isotonic_cls.joblib\")\n","\n","        # Load ensemble config\n","        self.cfg = json.load(open(ENS_DIR / \"inference_config.json\"))\n","        self.threshold = float(self.cfg[\"threshold\"])\n","        self.abstain_l, self.abstain_h = [float(x) for x in self.cfg[\"abstain_band\"]]\n","        self.w_sgd = float(self.cfg.get(\"w_sgd\", 1.0))\n","        self.w_nb  = float(self.cfg.get(\"w_nb\", 1.0))\n","\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.nb_model.to(self.device)\n","\n","    @staticmethod\n","    def _split(name: str):\n","        s = (name or \"\").strip().lower()\n","        parts = s.split()\n","        if len(parts) == 0: return \"\", \"\"\n","        if len(parts) == 1: return parts[0], \"\"\n","        return parts[0], parts[-1]\n","\n","    def _sgd_prob(self, name: str) -> float:\n","        first, last = self._split(name)\n","        Xfi = self.vec_first.transform([first])\n","        Xla = self.vec_last.transform([last])\n","        p_first = self.sgd_first.predict_proba(Xfi)[:, 1]\n","        p_last  = self.sgd_last.predict_proba(Xla)[:, 1]\n","        cp_first = np.clip(self.cal_first.predict(p_first), 0, 1)[0]\n","        cp_last  = np.clip(self.cal_last.predict(p_last),  0, 1)[0]\n","        # Probabilistic OR fusion\n","        return float(1.0 - (1.0 - cp_first) * (1.0 - cp_last))\n","\n","    def _nb_prob(self, name: str) -> float:\n","        first, last = self._split(name)\n","        text = f\"<FIRST> {first} <SEP> <LAST> {last}\"\n","        enc = self.tok(text, truncation=True, padding=\"max_length\",\n","                       max_length=self.MAX_LEN, return_tensors=\"pt\")\n","        with torch.no_grad():\n","            logits = self.nb_model(\n","                input_ids=enc[\"input_ids\"].to(self.device),\n","                attention_mask=enc[\"attention_mask\"].to(self.device)\n","            ).logits\n","            p = torch.softmax(logits, dim=-1)[:, 1].item()\n","        # Isotonic calibration (fit on validation earlier)\n","        return float(np.clip(self.iso_nb.predict([p])[0], 0, 1))\n","\n","    def predict_one(self, name: str,\n","                    override_threshold: Optional[float] = None,\n","                    override_abstain: Optional[List[float]] = None):\n","        ps = self._sgd_prob(name)\n","        pn = self._nb_prob(name)\n","        p  = (self.w_sgd * ps + self.w_nb * pn) / (self.w_sgd + self.w_nb)\n","\n","        thr = self.threshold if override_threshold is None else float(override_threshold)\n","        if override_abstain is None:\n","            a_low, a_high = self.abstain_l, self.abstain_h\n","        else:\n","            a_low, a_high = float(override_abstain[0]), float(override_abstain[1])\n","\n","        if p < a_low:\n","            decision = \"non_sa\"\n","        elif p > a_high:\n","            decision = \"sa\"\n","        else:\n","            decision = \"abstain\"\n","\n","        hard = \"sa\" if p >= thr else \"non_sa\"\n","        return {\n","            \"name\": name,\n","            \"prob_sgd\": ps,\n","            \"prob_nb\": pn,\n","            \"prob_ensemble\": p,\n","            \"decision_abstain_band\": decision,\n","            \"hard_decision_at_threshold\": {\"threshold\": thr, \"label\": hard},\n","        }\n","\n","# ---------- FastAPI app ----------\n","app = FastAPI(title=\"South Asian Name Classifier (Ensemble)\",\n","              version=\"1.0.0\",\n","              description=\"Calibrated ensemble of SGD fusion + NameBERT-SA with precision-first operating policy.\")\n","\n","PREDICTOR: Optional[EnsemblePredictor] = None\n","\n","class PredictRequest(BaseModel):\n","    name: str = Field(..., description=\"Full name string\")\n","    threshold: Optional[float] = Field(None, description=\"Override operating threshold (0..1)\")\n","    abstain_band: Optional[List[float]] = Field(None, description=\"[low, high] override for abstain band\")\n","\n","class PredictBatchRequest(BaseModel):\n","    names: List[str]\n","    threshold: Optional[float] = None\n","    abstain_band: Optional[List[float]] = None\n","\n","@app.on_event(\"startup\")\n","def _load():\n","    global PREDICTOR\n","    PREDICTOR = EnsemblePredictor()\n","\n","@app.get(\"/health\")\n","def health():\n","    return {\"status\": \"ok\"}\n","\n","@app.post(\"/predict\")\n","def predict(req: PredictRequest):\n","    out = PREDICTOR.predict_one(req.name, req.threshold, req.abstain_band)\n","    return out\n","\n","@app.post(\"/predict_batch\")\n","def predict_batch(req: PredictBatchRequest):\n","    results = [PREDICTOR.predict_one(n, req.threshold, req.abstain_band) for n in req.names]\n","    return {\"results\": results}"],"metadata":{"id":"gso0beZspILS","executionInfo":{"status":"aborted","timestamp":1758484271200,"user_tz":240,"elapsed":12,"user":{"displayName":"Aaron P","userId":"11425249210925611166"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How to run the service."],"metadata":{"id":"_r0pZsGt5RxO"}},{"cell_type":"code","source":["# (optional) create & activate a venv first\n","pip install fastapi uvicorn pydantic torch transformers joblib numpy\n","\n","# from the directory that contains the artifacts_* folders and inference_service.py:\n","uvicorn inference_service:app --host 0.0.0.0 --port 8000"],"metadata":{"id":"RKz0zkXv5WZD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Examples of how to request jobs from the service can be found below.:"],"metadata":{"id":"lHuK6RsI5dgQ"}},{"cell_type":"code","source":["# Single prediction\n","curl -X POST http://localhost:8000/predict \\\n","  -H \"Content-Type: application/json\" \\\n","  -d '{\"name\": \"Daniel Singh\"}'\n","\n","# Or with a batch of names\n","curl -X POST http://localhost:8000/predict_batch \\\n","  -H \"Content-Type: application/json\" \\\n","  -d '{\"names\": [\"Mary Thomas\", \"Noah Patel\", \"Kevin Johnson\"]}'"],"metadata":{"id":"x0Iq110s5eDt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utilizing the Model"],"metadata":{"id":"no-pEQw0QKMJ"}},{"cell_type":"markdown","source":["Download the batch_score.py file and The model artifacts folders: artifacts_tuned_sgd, artifacts_namebert_sa, artifacts_ensemble. Then in that directory run the following in the command line or terminal."],"metadata":{"id":"5YPxRF14Qf6S"}},{"cell_type":"markdown","source":["Or you could launch an API service."],"metadata":{"id":"5bF4gndM4qQ6"}},{"cell_type":"code","source":[],"metadata":{"id":"9JG5pFLt5RHU"},"execution_count":null,"outputs":[]}]}